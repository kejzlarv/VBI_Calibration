{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Required packages\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyDOE as doe\n",
    "import sklearn.gaussian_process.kernels as skl\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import theano\n",
    "import time\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "#import sys\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "#import psutil\n",
    "import os\n",
    "#import shutil\n",
    "import copy\n",
    "from matplotlib import gridspec\n",
    "from scipy.stats import multivariate_normal\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "#from scipy.optimize import minimize\n",
    "#from scipy.optimize import fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Function defs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##### Run this block before any simulation or LDM code #####\n",
    "def input_locations(d_input, n_input, lims, criterion = \"c\"): \n",
    "    \"\"\"Simulation inputs generator\n",
    "    \n",
    "    The function generates inputs for model/observations according to latin hypercube design or uniform design\n",
    "    within given range and dimension of inputs\n",
    "    \n",
    "    Args:\n",
    "        d_input: The dimension of inputs to be generated\n",
    "        n_input: The number of inputs\n",
    "        lims: Ndarray with the range of inputs for each variable , each row is limits for each var\n",
    "        criterion: \"m\", \"c\" for lating hypercube, \"uniform\" for uniform design\n",
    "        \n",
    "    Returns:\n",
    "        inputs: n_input * d_input Ndarray with generated input points.\n",
    "    \"\"\"\n",
    "    if criterion == \"uniform\":\n",
    "        inputs = []\n",
    "        for i in range(d_input):\n",
    "            inputs = inputs + [np.linspace(lims[i, 0], lims[i, 1], int(n_input ** (1 / d_input)))]\n",
    "            \n",
    "        grid_list = np.meshgrid(*inputs)\n",
    "        \n",
    "        inputs = np.zeros((n_input, d_input))\n",
    "        for i in range(d_input):\n",
    "            inputs[:,i] = grid_list[i].flatten()        \n",
    "        \n",
    "    else:\n",
    "        # Data generation\n",
    "        inputs = doe.lhs(d_input, samples = n_input, criterion = criterion)\n",
    "\n",
    "        # Transformation for the interval\n",
    "        for i in range(d_input):\n",
    "            inputs[:,i] = inputs[:,i] * (lims[i,1] - lims[i, 0]) + lims[i, 0]\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def gp_mean_cov(input_obs, input_m, d_input, theta, kernels, means):\n",
    "    \"\"\"Creates mean vector and covariance matrix based on GP specifications for simulation\n",
    "    \n",
    "    The function generates the mean and convariance function of the prior GP that is \n",
    "    to be used for data generation of observations Y and model evaluations Z according to\n",
    "    Kennedy & O'Hagan (2001) framework to calibration.\n",
    "    \n",
    "    Args:\n",
    "        input_obs: Ndarray with observation inputs.\n",
    "        input_m: Ndarray with model inputs.\n",
    "        d_input: The dimension of model inputs.\n",
    "        theta: True value of calibration parameter\n",
    "        kernels: A dictionary containing 3 items: f, delta, and noise kernels\n",
    "        means: A dictionary containing 2 items: f, delta\n",
    "                \n",
    "    Returns:\n",
    "        M: Mean vector of MVN distribution\n",
    "        K: Covariance matrix of MVN distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    ##### Covariance part\n",
    "    # Kernel input\n",
    "    theta_stack = np.repeat(theta, len(input_obs))\n",
    "    theta_stack = theta_stack.reshape((len(input_obs), input_m.shape[1] - d_input), order='F')\n",
    "    kernel_input = np.concatenate((np.concatenate([input_obs, theta_stack], axis = 1), input_m), axis = 0)\n",
    "    \n",
    "    # Base kernel without noise for the model GP\n",
    "    kernel_base = kernels[\"f\"]\n",
    "    K_base = kernel_base(kernel_input)\n",
    "    \n",
    "    # Model discrepancy kernel\n",
    "    if kernels[\"delta\"] != 0:\n",
    "        \n",
    "        zeros = np.zeros((input_m.shape[0], input_obs.shape[1]))\n",
    "        delta_input = np.concatenate([input_obs, zeros])\n",
    "        K_delta = kernels[\"delta\"](delta_input)\n",
    "        # Need to set the non-obs parts of the kernel to be zero\n",
    "        K_delta[:, input_obs.shape[0]:] = 0\n",
    "        K_delta[input_obs.shape[0]:, :] = 0\n",
    "        K_base = K_base + K_delta\n",
    "        \n",
    "    # Add noise to the observations\n",
    "    K = K_base + np.diag([kernels[\"sigma\"] ** 2] * input_obs.shape[0] + [0] * input_m.shape[0])\n",
    "    \n",
    "    ##### Means part\n",
    "\n",
    "    # Obs mean\n",
    "    obs_mean = np.array([])\n",
    "    input_obs_theta = np.concatenate([input_obs, theta_stack], axis = 1)\n",
    "    for elem in input_obs_theta:\n",
    "        # First input to mean is x, second input is theta\n",
    "        obs_mean = np.append(obs_mean, means[\"f\"](elem[:d_input], elem[d_input:]) + means[\"delta\"](elem[:d_input]))\n",
    "    \n",
    "    # Model mean\n",
    "    m_mean = np.array([])\n",
    "    for elem in input_m:\n",
    "        # First input to mean is x, second input is theta\n",
    "        m_mean = np.append(m_mean, means[\"f\"](elem[:d_input], elem[d_input:]))\n",
    "\n",
    "    M = np.concatenate((obs_mean, m_mean))\n",
    "    \n",
    "    return M, K\n",
    "\n",
    "def bijection_array(k, n, l): \n",
    "    \"\"\"Calculates bijection of k in range 1,..., l * (2 * n - (l + 1)) / 2 \n",
    "    mapped onto (i,j) where i in 1,..., n-j and j in 1,...,l\n",
    "    \n",
    "    Args:\n",
    "        k: input\n",
    "        n: number of observations\n",
    "        l: truncation level of a vine\n",
    "    \n",
    "    Returns:\n",
    "        i: index i\n",
    "        j: index j\n",
    "    \"\"\"\n",
    "        \n",
    "    bijection_domain = int(round(l * (2 * n - (l + 1)) / 2))\n",
    "\n",
    "    j = 0\n",
    "    int_high_prev = 0\n",
    "    int_high = 0\n",
    "    while k > int_high:\n",
    "        j = j + 1\n",
    "        int_high_prev = int_high\n",
    "        int_high = int_high + n - j\n",
    "\n",
    "    i = k - int_high_prev\n",
    "    \n",
    "    return i, j\n",
    "\n",
    "def partial_corr(i_low, i_hig, cond_list, data_input, kernels_input, corr_dict): \n",
    "    \"\"\"Function calculates partial correlation rho_{i_low, i_hig: cond_list}\n",
    "    based on the recursive formula. Here we assume i_low < i_hig and cond_list is\n",
    "    ordered.\n",
    "    \n",
    "    Note:\n",
    "    This is a utility function for the master functions defined below\n",
    "    \n",
    "    Args:\n",
    "        i_low: First of the conditioned variables for partial correlation\n",
    "        i_hig: Second of the conditioned variables for partial correlation\n",
    "        cond_list: An ordered list of conditioning variables\n",
    "        data_input: A dictionary containing 3 items: Response, X, theta for each of data points\n",
    "        kernels_input: A dictionary containing 3 items: f, delta, and noise kernels\n",
    "        corr_dict: a dictionary that is to be filled with calculated partial correlation\n",
    "        \n",
    "        NOTE: the entries in corr_dict have the following keys (strings) \n",
    "                i_low,i_high:cond_list[0],cond_list[-1] for conditioning list of size at leas 2\n",
    "                i_low,i_high:cond_list[-1] for conditioning list of size 1\n",
    "                i_low,i_high for conditioning list of size 0\n",
    "    Returns:\n",
    "        rho: Calculated partial correlation\n",
    "        corr_dict: Updated correlation dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if len(cond_list) > 0:\n",
    "        \n",
    "        corr_id = str(int(i_low)) + \",\" + str(int(i_hig)) + \":\"\n",
    "        if len(cond_list) > 1:\n",
    "            corr_id = corr_id + str(int(cond_list[0])) + \",\" + str(int(cond_list[-1]))\n",
    "        else:\n",
    "            corr_id = corr_id + str(int(cond_list[0]))\n",
    "            \n",
    "        if corr_id not in corr_dict: \n",
    "            v = cond_list[-1]\n",
    "            cond_list = cond_list[:-1]\n",
    "\n",
    "            rho_i_j, corr_dict = partial_corr(i_low, i_hig, cond_list, data_input, kernels_input, corr_dict)\n",
    "\n",
    "            rho_i_v, corr_dict = partial_corr(int(min(i_low,v)) , int(max(i_low,v)), cond_list,\n",
    "                                                data_input, kernels_input, corr_dict)\n",
    "\n",
    "            rho_v_j, corr_dict = partial_corr(int(min(i_hig,v)) , int(max(i_hig,v)), cond_list,\n",
    "                                                data_input, kernels_input, corr_dict)\n",
    "\n",
    "            rho = (rho_i_j - rho_i_v * rho_v_j) / np.sqrt((1 - rho_i_v ** 2) * (1 - rho_v_j ** 2))\n",
    "            corr_dict[corr_id] = rho\n",
    "        else:\n",
    "            rho = corr_dict[corr_id]    \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        corr_id = str(int(i_low)) + \",\" + str(int(i_hig))\n",
    "        if corr_id not in corr_dict: \n",
    "                # All the neccessary inputs\n",
    "            i_low_type = data_input[\"Response\"][i_low - 1,:][1]\n",
    "            i_hig_type = data_input[\"Response\"][i_hig - 1,:][1]\n",
    "\n",
    "            i_low_x = data_input[\"X\"][i_low - 1,1:] \n",
    "            i_hig_x = data_input[\"X\"][i_hig - 1,1:] \n",
    "\n",
    "            i_low_x_theta = np.concatenate((i_low_x, data_input[\"Theta\"][i_low - 1,1:]))\n",
    "            i_hig_x_theta = np.concatenate((i_hig_x, data_input[\"Theta\"][i_hig - 1,1:]))\n",
    "\n",
    "            # Here we need to distinguis between cases\n",
    "\n",
    "            if (i_low_type == 1) & (i_hig_type == 0):\n",
    "                # low is exp\n",
    "                # high is model   \n",
    "\n",
    "                #variances\n",
    "                var_low = kernels_input[\"sigma\"] ** 2\n",
    "                if kernels_input[\"f\"] != 0:\n",
    "                    var_low = var_low + float(kernels_input[\"f\"]([i_low_x_theta])) \n",
    "                    \n",
    "                if kernels_input[\"delta\"] != 0:\n",
    "                    var_low = var_low + float(kernels_input[\"delta\"]([i_low_x]))\n",
    "                var_hig = float(kernels_input[\"f\"]([i_hig_x_theta]))\n",
    "\n",
    "                #correlation\n",
    "                cov = float(kernels_input[\"f\"](np.array([i_low_x_theta, i_hig_x_theta]))[0,1])\n",
    "                rho = cov / (np.sqrt(var_hig * var_low))\n",
    "\n",
    "            elif (i_low_type == 0) & (i_hig_type == 1):\n",
    "                # low is model\n",
    "                # hig is exp\n",
    "\n",
    "                var_hig = float(kernels_input[\"f\"]([i_hig_x_theta])) + kernels_input[\"sigma\"] ** 2\n",
    "                if kernels_input[\"delta\"] != 0:\n",
    "                    var_hig = var_hig + float(kernels_input[\"delta\"]([i_hig_x]))\n",
    "                var_low = float(kernels_input[\"f\"]([i_low_x_theta]))\n",
    "\n",
    "                #correlation\n",
    "                cov = float(kernels_input[\"f\"](np.array([i_low_x_theta, i_hig_x_theta]))[0,1])\n",
    "                rho = cov / (np.sqrt(var_hig * var_low))\n",
    "\n",
    "            elif (i_low_type == 0) & (i_hig_type == 0):\n",
    "                # low is model\n",
    "                # hig is model\n",
    "\n",
    "                var_low = float(kernels_input[\"f\"]([i_low_x_theta]))\n",
    "                var_hig = float(kernels_input[\"f\"]([i_hig_x_theta]))\n",
    "\n",
    "                #correlation\n",
    "                cov = float(kernels_input[\"f\"](np.array([i_low_x_theta, i_hig_x_theta]))[0,1])\n",
    "                rho = cov / (np.sqrt(var_hig * var_low))\n",
    "\n",
    "            elif (i_low_type == 1) & (i_hig_type == 1):\n",
    "                # low is exp\n",
    "                # hig is exp\n",
    "\n",
    "                #variances and covariances\n",
    "                var_low = kernels_input[\"sigma\"] ** 2\n",
    "                var_hig = kernels_input[\"sigma\"] ** 2\n",
    "                cov = 0\n",
    "                if kernels_input[\"f\"] != 0:\n",
    "                    var_low = float(kernels_input[\"f\"]([i_low_x_theta])) + var_low\n",
    "                    var_hig = float(kernels_input[\"f\"]([i_hig_x_theta])) + var_hig\n",
    "                    cov = float(kernels_input[\"f\"](np.array([i_low_x_theta, i_hig_x_theta]))[0,1])\n",
    "                if kernels_input[\"delta\"] != 0:\n",
    "                    var_low = var_low + float(kernels_input[\"delta\"]([i_low_x]))\n",
    "                    var_hig = var_hig + float(kernels_input[\"delta\"]([i_hig_x]))\n",
    "                    cov = float(cov + kernels_input[\"delta\"](np.array([i_low_x, i_hig_x]))[0,1])\n",
    "\n",
    "                rho = cov / (np.sqrt(var_hig * var_low))\n",
    "\n",
    "            corr_dict[corr_id] = rho\n",
    "        else:\n",
    "            rho = corr_dict[corr_id]\n",
    "    return rho, corr_dict\n",
    "\n",
    "\n",
    "def partial_corr_c_master(i, j, data_input, kernels_input):\n",
    "    \"\"\"Function calculates partial correlation for the C-vine case.\n",
    "    \n",
    "    NOTE: This function is slightly redundant, I am not sure if I will actually need to use it\n",
    "    \n",
    "    Args:\n",
    "        i: Index i from the bijection\n",
    "        j: Index j from the bijection\n",
    "        data_input: A dictionary containing 3 items: Response, X, theta for each of data points\n",
    "        kernels_input: A dictionary containing 3 items: f, delta, and noise kernels\n",
    "        \n",
    "    Returns:\n",
    "        rho: Calculated partial correlation for a Gaussian C-Vine to be calucalted\n",
    "        corr_dict: A dictionary with all the partial correlations obrained through the recursion.\n",
    "        \n",
    "    NOTE: See partial_corr function for beter description of corr_dict\n",
    "    \"\"\"\n",
    "    \n",
    "    cond_list = [i + 1 for i in range(j - 1)]\n",
    "    corr_dict = {}\n",
    "    i_low = j\n",
    "    i_hig = j + i\n",
    "    \n",
    "    rho, corr_dict = partial_corr(i_low, i_hig, cond_list, data_input, kernels_input, corr_dict)\n",
    "    return rho, corr_dict\n",
    "\n",
    "def partial_corr_d_master(i, j, data_input, kernels_input): \n",
    "    \"\"\"Function calculates partial correlation for the D-vine case.\n",
    "    \n",
    "    NOTE: This function is slightly redundant, I am not sure if I will actually need to use it\n",
    "    \n",
    "    Args:\n",
    "        i: Index i from the bijection\n",
    "        j: Index j from the bijection\n",
    "        data_input: A dictionary containing 3 items: Response, X, theta for each of data points\n",
    "        kernels_input: A dictionary containing 3 items: f, delta, and noise kernels\n",
    "        \n",
    "    Returns:\n",
    "        rho: Calculated partial correlation for a Gaussian D-Vine to be calucalted\n",
    "        corr_dict: A dictionary with all the partial correlations obrained through the recursion.\n",
    "        \n",
    "    NOTE: See partial_corr function for beter description of corr_dict\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    cond_list = [i for i in range(i + 1 ,i + j)]\n",
    "    corr_dict = {}\n",
    "    i_low = i\n",
    "    i_hig = j + i\n",
    "\n",
    "    rho, corr_dict = partial_corr(i_low, i_hig, cond_list, data_input, kernels_input, corr_dict)\n",
    "    return rho, corr_dict\n",
    "\n",
    "def bivariate_gauss_copula(i, j, data_input, kernels_input, means_input, vine_type = \"C\"): \n",
    "    \"\"\"Evaluates bivariate gaussian copula density for given values of i,j and vine copula type.\n",
    "    \n",
    "    Args:\n",
    "        i: Output from bijection that uniquely identifies first datapoint\n",
    "        j: Output from bijection that uniquely identifies second datapoint\n",
    "        data_input: A dictionary containing 3 items: Response, X, theta for each of data points\n",
    "        kernels_input: A dictionary containing 3 items: f, delta, and noise kernels\n",
    "        means_input: A dictionary containing 2 items: f, delta\n",
    "        vine_type: \"C\" or \"D\" depending on which wine copule we want to consider\n",
    "        \n",
    "    Returns:\n",
    "        copula_density_eval: Evaluated bivariate gaussian copula    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculating the corr_dict\n",
    "    if vine_type == \"C\":\n",
    "        cond_list = [i + 1 for i in range(j - 1)]\n",
    "        i_low = j\n",
    "        i_hig = j + i\n",
    "        rho, corr_dict = partial_corr_c_master(i, j, data_input, kernels_input)\n",
    "    elif vine_type == \"D\":\n",
    "        cond_list = [i for i in range(i + 1 ,i + j)]\n",
    "        i_low = i\n",
    "        i_hig = j + i\n",
    "        rho, corr_dict = partial_corr_d_master(i, j, data_input, kernels_input)\n",
    "        \n",
    "    # Based on a truncation level, what is the size of conditioning set?\n",
    "    if len(cond_list) > 0:\n",
    "\n",
    "        v = cond_list[-1]\n",
    "        cond_list_h = cond_list[:-1]\n",
    "\n",
    "        F_low = h_function(i_low, v, cond_list_h, data_input, kernels_input, means_input, corr_dict)\n",
    "        F_hig = h_function(i_hig, v, cond_list_h, data_input, kernels_input, means_input, corr_dict)\n",
    "\n",
    "        corr_low = min(i_low, i_hig)\n",
    "        corr_hig = max(i_low, i_hig)\n",
    "        \n",
    "        # conditional indicator\n",
    "        corr_id = str(int(corr_low)) + \",\" + str(int(corr_hig)) + \":\" + str(int(cond_list[0]))\n",
    "        if len(cond_list) > 1:\n",
    "             corr_id = corr_id + \",\" + str(int(v))\n",
    "\n",
    "        rho = corr_dict[corr_id]\n",
    "\n",
    "    else:\n",
    "        Y_low_std, Y_hig_std, var_low, var_hig, i_low_type, i_hig_type = std_transform(i_low, i_hig, data_input, kernels_input, means_input)\n",
    "\n",
    "        F_low = sp.stats.norm.cdf(Y_low_std)\n",
    "        F_hig = sp.stats.norm.cdf(Y_hig_std)\n",
    " \n",
    "\n",
    "        corr_low = min(i_low, i_hig)\n",
    "        corr_hig = max(i_low, i_hig)\n",
    "\n",
    "        # conditional indicator\n",
    "        corr_id = str(int(corr_low)) + \",\" + str(int(corr_hig))\n",
    "        rho = corr_dict[corr_id]\n",
    "\n",
    "    copula_density_eval = b_gaussian_c_pdf(F_low, F_hig, rho)\n",
    "    \n",
    "    return copula_density_eval\n",
    "\n",
    "def h_function(i_low, i_hig, cond_list, data_input, kernels_input, means_input, corr_dict): #Works for general case\n",
    "    \"\"\"Calculates h(u_{i_low}, u_{i_hig}) parameterized by rho_{i_low,i_hig:cond_list} recuresively\n",
    "    \n",
    "    NOTE: i_low, i_hig have slightly different meaning that in the case of partial correlation\n",
    "    because here oreder matters so i_low, i_hig here correspinds to ored of arguments\n",
    "    \n",
    "    Args:\n",
    "        i_low: First argument index for h\n",
    "        i_hig: Second argument index for h\n",
    "        cond_list: A list of conditioning variables for h viz rho in description\n",
    "        data_input: A dictionary containing 3 items: Response, X, theta for each of data points\n",
    "        kernels_input: A dictionary containing 3 items: f, delta, and noise kernels\n",
    "        means_input: A dictionary containing 2 items: f, delta\n",
    "        corr_dict: A dictionary containg all the partial correlations precalculated using parial_corr fcn\n",
    "        \n",
    "    Returns:\n",
    "        h: Calculated value of h function\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(cond_list) > 0:\n",
    "\n",
    "        v = cond_list[-1]\n",
    "        cond_list_h = cond_list[:-1]\n",
    "        \n",
    "        F_low = h_function(i_low, v, cond_list_h, data_input, kernels_input, means_input, corr_dict)\n",
    "        F_hig = h_function(i_hig, v, cond_list_h, data_input, kernels_input, means_input, corr_dict)\n",
    "        \n",
    "        corr_low = min(i_low, i_hig)\n",
    "        corr_hig = max(i_low, i_hig)\n",
    "        \n",
    "        # conditional indicator \n",
    "        corr_id = str(int(corr_low)) + \",\" + str(int(corr_hig)) + \":\" + str(int(cond_list[0]))\n",
    "        if len(cond_list) > 1:\n",
    "            corr_id = corr_id + \",\" + str(int(v))\n",
    "\n",
    "        rho = corr_dict[corr_id]\n",
    "        h = sp.stats.norm.cdf((sp.stats.norm.ppf(F_low) - rho * sp.stats.norm.ppf(F_hig)) / np.sqrt(1 - rho ** 2))\n",
    "             \n",
    "    else:\n",
    "            \n",
    "        Y_low_std, Y_hig_std, var_low, var_hig, i_low_type, i_hig_type = std_transform(i_low, i_hig, data_input, kernels_input, means_input)\n",
    "\n",
    "        corr_low = min(i_low, i_hig)\n",
    "        corr_hig = max(i_low, i_hig)\n",
    "\n",
    "        \n",
    "        rho = corr_dict[str(int(corr_low)) + \",\" + str(int(corr_hig))]\n",
    "        h = sp.stats.norm.cdf((Y_low_std - rho * Y_hig_std) / np.sqrt(1 - rho ** 2))\n",
    "        \n",
    "    return h\n",
    "\n",
    "def vine_p(i, j, l, n, data_input, kernels_input, means_input, vine_type = \"C\"):\n",
    "    \"\"\"Function calculates the p (as defined in Kejzlar and Maiti (2020))\n",
    "    function based on the vine copula with given truncation\n",
    "    \n",
    "    Note:\n",
    "    - this is the \"p\" for the Algorithm 1 and for Algorithm 1 + Control Variates \n",
    "    - p under Rao-Blackwellization defined below\n",
    "    - this is version without Rao-Blackwellization\n",
    "    \n",
    "    Args:\n",
    "        i: Index i from the bijection\n",
    "        j: Index j from the bijection\n",
    "        l: Truncation level\n",
    "        n: Sample size\n",
    "        data_input: A dictionary containing 3 items: Response, X, theta for each of data points\n",
    "        kernels_input: A dictionary containing 3 items: f, delta, and noise kernels\n",
    "        means_input: A dictionary containing 2 items: f, delta\n",
    "        vine_type: \"C\" or \"D\" depending on which wine copule we want to consider\n",
    "        \n",
    "    Returns:\n",
    "        p_eval: evaluated p_function\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if vine_type == \"D\":\n",
    "        if j > l: # This conditional is unnecesary since it is taken care of by the bijeciton function\n",
    "            c_i_j = 1.0\n",
    "        else:\n",
    "            c_i_j = bivariate_gauss_copula(i, j, data_input, kernels_input, means_input, vine_type = \"D\")\n",
    "\n",
    "        # definition of a_i multiplicatior\n",
    "        a = 2 * l\n",
    "        if i <= l:\n",
    "            a = a - (l + 1 - i)\n",
    "        if i > (n - l):\n",
    "            a = a - (l - n + i)\n",
    "\n",
    "        # definition of b_{i+j} multiplicator\n",
    "        b = 2 * l\n",
    "        if (i + j) <= l:\n",
    "            b = b - (l + 1 - j - i)\n",
    "        if (i + j) > (n - l):\n",
    "            b = b - (l - n + j + i)\n",
    "\n",
    "        # Standardized data\n",
    "        Y_low_std, Y_hig_std, var_low, var_hig, i_low_type, i_hig_type  = std_transform(i, j + i, data_input, kernels_input, means_input)\n",
    "    else:\n",
    "        if j > l: # See above\n",
    "            c_i_j = 1.0\n",
    "        else:\n",
    "            c_i_j = bivariate_gauss_copula(i, j, data_input, kernels_input, means_input, vine_type = \"C\")\n",
    "\n",
    "        # definition of a_i multiplier \n",
    "        a = n - 1\n",
    "\n",
    "        # definition of b_{j+i}\n",
    "        b = l\n",
    "        if (j + i) <= l:\n",
    "            b = b + (n - 1 - l)\n",
    "        \n",
    "        \n",
    "        # Standardized data\n",
    "        Y_low_std, Y_hig_std, var_low, var_hig, i_low_type, i_hig_type = std_transform(j, j + i, data_input, kernels_input, means_input)\n",
    "        \n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    \n",
    "    p_eval = np.log(c_i_j) + (1 / a) * np.log(sp.stats.norm.pdf(Y_low_std)) + (1 / b) * np.log(sp.stats.norm.pdf(Y_hig_std))\n",
    "\n",
    "    return p_eval\n",
    "\n",
    "def data_type(i_low, i_hig, data_input):\n",
    "    \"\"\"Function extracts the type of data indexed by i_low and i_hig\n",
    "    \n",
    "     Args:\n",
    "        i_low: First argument index\n",
    "        i_hig: Second argument index\n",
    "        data_input: A dictionary containing 3 items: Response, X, theta for each of data points\n",
    "               \n",
    "    Returns:\n",
    "        i_low_type: 1 for y, 0 for z\n",
    "        i_hig_type: 1 for y, 0 for z\n",
    "    \"\"\"\n",
    "    \n",
    "    i_low_type = data_input[\"Response\"][i_low - 1,:][1]\n",
    "    i_hig_type = data_input[\"Response\"][i_hig - 1,:][1]\n",
    "    \n",
    "    return i_low_type, i_hig_type\n",
    "\n",
    "def std_transform(i_low, i_hig, data_input, kernels_input, means_input): \n",
    "    \"\"\"Standardize the data points according to the GP specification.\n",
    "    \n",
    "    Args:\n",
    "        i_low: First argument index\n",
    "        i_hig: Second argument index\n",
    "        data_input: A dictionary containing 3 items: Response, X, theta for each of data points\n",
    "        kernels_input: A dictionary containing 3 items: f, delta, and noise kernels\n",
    "        means_input: A dictionary containing 2 items: f, delta\n",
    "            \n",
    "    Returns:\n",
    "        Y_low_std: Standardized value with index i_low\n",
    "        Y_hig_std: Standardized value with index i_hig \n",
    "        var_low: Variance of data point with index i_low\n",
    "        var_hig: Variance of data point with index i_hig\n",
    "        i_low_type: 1 for y, 0 for z\n",
    "        i_hig_type: 1 for y, 0 for z\n",
    "    \"\"\"\n",
    "    \n",
    "    i_low_type = data_input[\"Response\"][i_low - 1,:][1]\n",
    "    i_hig_type = data_input[\"Response\"][i_hig - 1,:][1]\n",
    "\n",
    "    i_low_x = data_input[\"X\"][i_low - 1,1:] \n",
    "    i_hig_x = data_input[\"X\"][i_hig - 1,1:]\n",
    "\n",
    "    i_low_y = data_input[\"Response\"][i_low - 1,2]\n",
    "    i_hig_y = data_input[\"Response\"][i_hig - 1,2]\n",
    "\n",
    "    i_low_theta = data_input[\"Theta\"][i_low - 1,1:]\n",
    "    i_hig_theta = data_input[\"Theta\"][i_hig - 1,1:]\n",
    "\n",
    "    # The basic simulation considers kernels without theta in them!\n",
    "    i_low_x_theta = np.concatenate((i_low_x, data_input[\"Theta\"][i_low - 1,1:]))\n",
    "    i_hig_x_theta = np.concatenate((i_hig_x, data_input[\"Theta\"][i_hig - 1,1:]))\n",
    "\n",
    "        # Uniform inputs calculation\n",
    "\n",
    "    if i_low_type == 1:\n",
    "            # low is exp      \n",
    "            # Varince\n",
    "        var_low = kernels_input[\"sigma\"] ** 2\n",
    "        if kernels_input[\"f\"] != 0:\n",
    "            var_low = var_low + float(kernels_input[\"f\"]([i_low_x_theta]))\n",
    "        if kernels_input[\"delta\"] != 0:\n",
    "              var_low = float(var_low + float(kernels_input[\"delta\"]([i_low_x])))\n",
    "            # Mean\n",
    "        mean_low = float(means_input[\"f\"](i_low_x, i_low_theta) + means_input[\"delta\"](i_low_x))\n",
    "\n",
    "    elif i_low_type == 0:\n",
    "            # low is model      \n",
    "            # Variance\n",
    "        var_low = float(kernels_input[\"f\"]([i_low_x_theta]))\n",
    "            # Mean\n",
    "        mean_low = float(means_input[\"f\"](i_low_x, i_low_theta))\n",
    "\n",
    "    if i_hig_type == 1:\n",
    "            # hig is exp     \n",
    "            # Varince\n",
    "        var_hig = kernels_input[\"sigma\"] ** 2\n",
    "        if kernels_input[\"f\"] != 0:\n",
    "            var_hig = float(kernels_input[\"f\"]([i_hig_x_theta])) + var_hig\n",
    "        if kernels_input[\"delta\"] != 0:\n",
    "            var_hig = float(var_hig + float(kernels_input[\"delta\"]([i_hig_x])))\n",
    "            # Mean\n",
    "        mean_hig = float(means_input[\"f\"](i_hig_x, i_hig_theta) + means_input[\"delta\"](i_hig_x))\n",
    "\n",
    "    elif i_hig_type == 0:\n",
    "            # low is model    \n",
    "            # Variance\n",
    "        var_hig = float(kernels_input[\"f\"]([i_hig_x_theta]))\n",
    "            # Mean\n",
    "        mean_hig = float(means_input[\"f\"](i_hig_x, i_hig_theta))\n",
    "        \n",
    "        # USE logarith for stability\n",
    "    Y_low_std = np.sign(i_low_y - mean_low) * np.exp(np.log(np.abs(i_low_y - mean_low)) - np.log(var_low) / 2)\n",
    "    Y_hig_std = np.sign(i_hig_y - mean_hig) * np.exp(np.log(np.abs(i_hig_y - mean_hig)) - np.log(var_hig) / 2)\n",
    "        \n",
    "    return Y_low_std, Y_hig_std, var_low, var_hig, i_low_type, i_hig_type\n",
    "\n",
    "def b_gaussian_c_pdf(u1, u2, rho):\n",
    "    \"\"\"Calculates the actual bivariate gaussian copula density.\n",
    "    \n",
    "    Args:\n",
    "        u1: The value of cdf for first arg\n",
    "        u2: the value of cdf for second arg\n",
    "        \n",
    "    Returns:\n",
    "        density: Evaluated bivariate gaussian copula density    \n",
    "    \"\"\"\n",
    "    w1 = sp.stats.norm.ppf(u1)\n",
    "    w2 = sp.stats.norm.ppf(u2)\n",
    "    \n",
    "    density = (1 / np.sqrt(1 - rho ** 2)) * np.exp(- ((rho ** 2) * (w1 ** 2 + w2 ** 2) - 2 * rho * w1 * w2) / (2 * (1 - rho ** 2)))\n",
    "    return density\n",
    "\n",
    "##### Distribution family classes that are used as 1) Variational fmailies, 2) Overdispersed familiees, 3) Priors\n",
    "# Each class contains the following methods:\n",
    "#\n",
    "# __init__(self, param, init)\n",
    "#\n",
    "# sample(self, n_size)\n",
    "#\n",
    "# log_pdf(self, theta)\n",
    "#\n",
    "# log_grad_pdf(self, theta)\n",
    "\n",
    "class gaussian_mean_field_family:\n",
    "    \"\"\"Class for a mean field gaussian family of arbitrary dimension\"\"\"\n",
    "    \n",
    "    def __init__(self, param, dim):\n",
    "        \"\"\"The parameters for the Gaussian mean field family are passed as a dictionary\n",
    "        \n",
    "        Args:\n",
    "            param: A dictionaray containing two entries with arrays of dimension (dim,)\n",
    "                 - param[\"mu\"] is a 1-dim Ndarray with the means of independent Gaussian distributions\n",
    "                 - param[\"sigma\"] is a 1-dim Ndarray with the np.log(SDs) of independent Gaussian distributions\n",
    "            dim: dimension of the family\n",
    "        \"\"\"\n",
    "        \n",
    "        self.mu = param[\"mu\"]\n",
    "        self.sigma = param[\"sigma\"]\n",
    "        self.dim = dim\n",
    "        \n",
    "    def sample(self, n_size):\n",
    "        \"\"\"Samples from the gaussian mean_field variational family\n",
    "        \n",
    "        Args:\n",
    "            n_size: the number of samples to be generated\n",
    "        Returns:\n",
    "            samle: 2 dimensional NDarray where each row is a single sample from the variational family\n",
    "        \"\"\"\n",
    "        \n",
    "        sample = np.zeros((n_size, self.dim))\n",
    "        for i in range(self.dim):\n",
    "            sample[:, i] = np.random.normal(loc = self.mu[i], scale = self.sigma[i], size = n_size)     \n",
    "        return sample\n",
    "    \n",
    "    def log_pdf(self, theta):\n",
    "        \"\"\"Calculates the value of log density of the mean field gaussian family at theta\n",
    "        \n",
    "        Args:\n",
    "            theta: 1-dim Ndarray of theta values at which the log density will be evaluated\n",
    "        Returns:\n",
    "            log_dens: The value of log density at theta\n",
    "        \"\"\"\n",
    "        \n",
    "        log_dens = 0\n",
    "        for i in range(self.dim):\n",
    "            density = sp.stats.norm(loc = self.mu[i], scale = self.sigma[i])\n",
    "            log_dens = log_dens + np.log(density.pdf(theta[i]))       \n",
    "        return log_dens\n",
    "    \n",
    "    def grad_log_pdf(self, theta):\n",
    "        \"\"\"Calculates the value of score function at theta\n",
    "        \n",
    "        Args:\n",
    "            theta: 1-dim Ndarray of theta values at which the log gradient will be evaluated\n",
    "        Returns:\n",
    "            gradient: 1-dim Ndarray of len(2 * dim) where gradient[:dim] are the values of score function\n",
    "                    w.r.t mu and gradient[dim:] are the values of score function w.r.t sigma\n",
    "        \"\"\"\n",
    "        \n",
    "        gradient = np.zeros(self.dim * 2)\n",
    "        for i in range(self.dim):\n",
    "            # score function w.r.t. mean\n",
    "            gradient[i] = (theta[i] - self.mu[i]) / (self.sigma[i] ** 2)\n",
    "            # score function w.r.t. SD\n",
    "            gradient[i + self.dim] = - 1 / self.sigma[i] + ((theta[i] - self.mu[i]) ** 2) / (self.sigma[i] ** 3)\n",
    "        return gradient\n",
    "    \n",
    "class gamma_mean_field_family:\n",
    "    \"\"\"Class for a mean field gamma family of arbitrary dimension with np.log(param) parametrization\n",
    "      \n",
    "      Gamma parametrization:\n",
    "          \n",
    "          f(x|alpha, beta) = \\frac{beta ^ alpha}{Gamma(alpha)} * x ^{alpha - 1} * \\exp{- beta * x}\n",
    "          \n",
    "          E(X) = alpha / beta\n",
    "          Var(X) = alpha / (beta^2)\n",
    "          \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, param, dim):\n",
    "        \"\"\"The parameters for the gamma mean field family are passed as a dictionary\n",
    "        \n",
    "        Args:\n",
    "            param: A dictionaray containing two entries with arrays of dimension (dim,)\n",
    "                 - param[\"alpha\"] is a 1-dim Ndarray with the np.log(alpha) parameters of independent Gamma distributions\n",
    "                 - param[\"beta\"] is a 1-dim Ndarray with the np.log(beta) parameters of independent Gamma distributions\n",
    "            dim: Dimension of the family\n",
    "        \"\"\"\n",
    "        \n",
    "        self.alpha = param[\"alpha\"]\n",
    "        self.beta = param[\"beta\"]\n",
    "        self.dim = dim\n",
    "        \n",
    "    def sample(self, n_size):\n",
    "        \"\"\"Samples from the gamma mean field variational family\n",
    "        \n",
    "        Note: np.random.gamma has parametrization with shape = alpha, scale = 1 / beta\n",
    "        \n",
    "        Args:\n",
    "            n_size: the number of samples to be generated\n",
    "        Returns:\n",
    "            samle: 2 dimensional NDarray where each row is a single sample from the variational family\n",
    "        \"\"\"\n",
    "        \n",
    "        sample = np.zeros((n_size, self.dim))\n",
    "        for i in range(self.dim):\n",
    "            sample[:, i] = np.random.gamma(shape = self.alpha[i], scale = self.beta[i], size = n_size)\n",
    "        return sample\n",
    "    \n",
    "    def log_pdf(self, theta):\n",
    "        \"\"\"Calculates the value of log density of the mean field gamma family at theta\n",
    "        \n",
    "        Args:\n",
    "            theta: 1-dim Ndarray of theta values at which the log density will be evaluated\n",
    "        Returns:\n",
    "            log_dens: The value of log density at theta\n",
    "        \"\"\"\n",
    "        \n",
    "        log_dens = 0\n",
    "        for i in range(self.dim):\n",
    "            log_dens = log_dens + (self.alpha[i] - 1) * np.log(theta[i]) - theta[i] / self.beta[i] - self.alpha[i] * np.log(self.beta[i]) - np.log(sp.special.gamma(self.alpha[i]))\n",
    "    \n",
    "        return log_dens\n",
    "    \n",
    "    def grad_log_pdf(self, theta):\n",
    "        \"\"\"Calculates the value of score function at theta\n",
    "        \n",
    "        Args:\n",
    "            theta: 1-dim Ndarray of theta values at which the log gradient will be evaluated\n",
    "        Returns:\n",
    "            gradient: 1-dim Ndarray of len(2 * dim) where gradient[:dim] are the values of score function\n",
    "                    w.r.t alpha and gradient[dim:] are the values of score function w.r.t beta\n",
    "        \"\"\"\n",
    "        \n",
    "        gradient = np.zeros(self.dim * 2)\n",
    "        for i in range(self.dim):    \n",
    "            # score function w.r.t. alpha\n",
    "            gradient[i] = np.log(theta[i]) - np.log(self.beta[i]) - sp.special.digamma(self.alpha[i])\n",
    "            # score function w.r.t. beta\n",
    "            gradient[i + self.dim] = theta[i] / (self.beta[i] ** 2) - self.alpha[i] / self.beta[i]\n",
    "        return gradient\n",
    "    \n",
    "class gaussian_mean_field_family_lambda_param:\n",
    "    \"\"\"Class for a mean field gaussian family of arbitrary dimension with scale parametrization \n",
    "    \n",
    "    lambda = log(exp(sigma) - a)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, param, dim):\n",
    "        \"\"\"The parameters for the Gaussian mean field family aare passed as a dictionary\n",
    "        \n",
    "        Args:\n",
    "            param: A dictionaray containing two entries with arrays of dimension (dim,)\n",
    "                 - param[\"mu\"] is a 1-dim Ndarray with the means of independent Gaussian distributions\n",
    "                 - param[\"sigma\"] is a 1-dim Ndarray with the transformed sd of independent Gaussian distributions\n",
    "                 - param[\"a\"] see parametrization\n",
    "            dim: Dimension of the family\n",
    "        \"\"\"\n",
    "        \n",
    "        self.mu = param[\"mu\"]\n",
    "        self.sigma = param[\"sigma\"]\n",
    "        self.a = param[\"a\"]\n",
    "        self.dim = dim\n",
    "        \n",
    "    def sample(self, n_size):\n",
    "        \"\"\"Samples from the gaussian mean_field variational family\n",
    "        \n",
    "        Args:\n",
    "            n_size: the number of samples to be generated\n",
    "        Returns:\n",
    "            samle: 2 dimensional NDarray where each row is a single sample from the variational family\n",
    "        \"\"\"\n",
    "        \n",
    "        sample = np.zeros((n_size, self.dim))\n",
    "        for i in range(self.dim):\n",
    "            sd = np.log(np.exp(self.sigma[i]) + self.a)\n",
    "            sample[:, i] = np.random.normal(loc = self.mu[i], scale = sd, size = n_size)     \n",
    "        return sample\n",
    "    \n",
    "    def log_pdf(self, theta):\n",
    "        \"\"\"Calculates the value of log density of the mean field gaussian family at theta\n",
    "        \n",
    "        Args:\n",
    "            theta: 1-dim Ndarray of theta values at which the log density will be evaluated\n",
    "        Returns:\n",
    "            log_dens: The value of log density at theta\n",
    "        \"\"\"\n",
    "        \n",
    "        log_dens = 0\n",
    "        for i in range(self.dim):\n",
    "            sd = np.log(np.exp(self.sigma[i]) + self.a)\n",
    "            density = sp.stats.norm(loc = self.mu[i], scale = sd)\n",
    "            log_dens = log_dens + np.log(density.pdf(theta[i]))       \n",
    "        return log_dens\n",
    "    \n",
    "    def grad_log_pdf(self, theta):\n",
    "        \"\"\"Calculates the value of score function at theta\n",
    "        \n",
    "        Args:\n",
    "            theta: 1-dim Ndarray of theta values at which the log gradient will be evaluated\n",
    "        Returns:\n",
    "            gradient: 1-dim Ndarray of len(2 * dim) where gradient[:dim] are the values of score function\n",
    "                    w.r.t mu and gradient[dim:] are the values of score function w.r.t sigma\n",
    "        \"\"\"\n",
    "        \n",
    "        gradient = np.zeros(self.dim * 2)\n",
    "        for i in range(self.dim):\n",
    "            # score function w.r.t. mean\n",
    "            gradient[i] = - (self.mu[i] - theta[i]) / (np.log(np.exp(self.sigma[i]) + self.a) ** 2)\n",
    "            # score function w.r.t. lambda\n",
    "            gradient[i + self.dim] = np.exp(self.sigma[i]) * (- 1 / (np.log(self.a + np.exp(self.sigma[i])) * (self.a + np.exp(self.sigma[i]))) + \\\n",
    "                                                             ((self.mu[i] - theta[i]) ** 2) / ((self.a + np.exp(self.sigma[i])) * np.log(self.a + np.exp(self.sigma[i])) ** 3))\n",
    "        return gradient\n",
    "\n",
    "    \n",
    "class gamma_mean_field_family_lambda_param:\n",
    "    \"\"\"Class for a mean field gamma family of arbitrary, parametrized with mean and standard deviation\n",
    "    \n",
    "    \n",
    "    lambda_alfa = log(exp(alfa) - a)\n",
    "    lambda_beta = log(exp(alfa) - a)\n",
    "    \n",
    "    where alpha corresponds to the mean of the gamma distribution and beta corresponds to the std of the gamma dist\n",
    "      \n",
    "      Gamma parametrization:\n",
    "          \n",
    "          f(x|alpha, beta) = \\frac{beta ^ alpha}{Gamma(alpha)} * x ^{alpha - 1} * \\exp{- beta * x}\n",
    "          \n",
    "          E(X) = alpha / beta\n",
    "          Var(X) = alpha / (beta^2)\n",
    "          \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, param, dim):\n",
    "        \"\"\"The parameters for the gamma mean field family are passed as a dictionary\n",
    "        \n",
    "        Args:\n",
    "            param: A dictionaray containing two entries with arrays of dimension (dim,)\n",
    "                 - param[\"alpha\"] is a 1-dim Ndarray with the mean paramteres of the independent Gamma distribution\n",
    "                 - param[\"beta\"] is a 1-dim Ndarray with the stds of independent Gamma distributions\n",
    "                 - param[\"a\"] see parametrization\n",
    "            dim: Dimension of the family\n",
    "        \"\"\"\n",
    "        \n",
    "        self.alpha = param[\"alpha\"]\n",
    "        self.beta = param[\"beta\"]\n",
    "        self.a = param[\"a\"]\n",
    "        self.dim = dim\n",
    "        \n",
    "    def sample(self, n_size):\n",
    "        \"\"\"Samples from the gamma mean field variational family\n",
    "        \n",
    "        Note: np.random.gamma has parametrization with shape = alpha, scale = 1 / beta\n",
    "        \n",
    "        Args:\n",
    "            n_size: the number of samples to be generated\n",
    "        Returns:\n",
    "            samle: 2 dimensional NDarray where each row is a single sample from the variational family\n",
    "        \"\"\"\n",
    "        \n",
    "        sample = np.zeros((n_size, self.dim))\n",
    "        for i in range(self.dim):\n",
    "            mu = np.log(np.exp(self.alpha[i]) + self.a)\n",
    "            sigma = np.log(np.exp(self.beta[i]) + self.a)\n",
    "            alpha = float(mu/sigma) ** 2\n",
    "            beta = float(mu/ (sigma**2))\n",
    "            tfd = tfp.distributions\n",
    "            tfdGamma = tfd.Gamma(concentration=alpha, rate=beta)\n",
    "            samples = tfdGamma.sample(n_size)\n",
    "            sample[:, i] = samples.numpy()\n",
    "        return sample\n",
    "    \n",
    "    def log_pdf(self, theta):\n",
    "        \"\"\"Calculates the value of log density of the mean field gamma family at theta\n",
    "        \n",
    "        Args:\n",
    "            theta: 1-dim Ndarray of theta values at which the log density will be evaluated\n",
    "        Returns:\n",
    "            log_dens: The value of log density at theta\n",
    "        \"\"\"\n",
    "        \n",
    "        log_dens = 0\n",
    "        for i in range(self.dim):\n",
    "            \n",
    "            mu = np.log(np.exp(self.alpha[i]) + self.a)\n",
    "            sigma = np.log(np.exp(self.beta[i]) + self.a)\n",
    "            alpha = float(mu/sigma) ** 2\n",
    "            beta = float(mu/ (sigma**2))\n",
    "            tfd = tfp.distributions\n",
    "            tfdGamma = tfd.Gamma(concentration=alpha, rate=beta)\n",
    "            log_dens = log_dens + float(tfdGamma.log_prob(theta[i]))\n",
    "        return log_dens\n",
    "    \n",
    "    def grad_log_pdf(self, theta):\n",
    "        \"\"\"Calculates the value of score function at theta\n",
    "        \n",
    "        Args:\n",
    "            theta: 1-dim Ndarray of theta values at which the log gradient will be evaluated\n",
    "        Returns:\n",
    "            gradient: 1-dim Ndarray of len(2 * dim) where gradient[:dim] are the values of score function\n",
    "                    w.r.t alpha and gradient[dim:] are the values of score function w.r.t beta\n",
    "        \"\"\"\n",
    "        \n",
    "        gradient = np.zeros(self.dim * 2)\n",
    "        for i in range(self.dim):    \n",
    "            l_m = self.alpha[i]\n",
    "            l_s = self.beta[i]\n",
    "            # score function w.r.t. lambda_alpha\n",
    "            g1m = (np.log(self.a + np.exp(l_s)) ** 2) * (self.a + np.exp(l_m))\n",
    "            g2m = np.log(self.a + np.exp(l_m))\n",
    "            gradient[i] = (np.exp(l_m) * g2m - theta[i] * np.exp(l_m) - 2 * np.exp(l_m) * sp.special.digamma((g2m / np.log(self.a + np.exp(l_s))) ** 2) + \\\n",
    "                          2 * np.exp(l_m) * np.log(theta[i]) * g2m + 2 * np.log(g2m / (np.log(self.a + np.exp(l_s)) ** 2)) * np.exp(l_m) * g2m) / g1m\n",
    "            \n",
    "            # score function w.r.t. lambda_beta\n",
    "            g1l = (np.log(self.a + np.exp(l_s)) ** 3) * (self.a + np.exp(l_s))\n",
    "            g2l = np.log(self.a + np.exp(l_m)) ** 2\n",
    "            gradient[i + self.dim] = (2 * np.exp(l_s) * sp.special.digamma(g2l / (np.log(self.a + np.exp(l_s)) ** 2)) * g2l - \\\n",
    "                                     2 * np.log(np.log(self.a + np.exp(l_m)) / (np.log(self.a + np.exp(l_s)) ** 2)) * np.exp(l_s) * g2l -\\\n",
    "                                     2 * np.exp(l_s) * g2l - 2 * np.exp(l_s) * np.log(theta[i]) * g2l + 2 * theta[i] * np.exp(l_s) * np.log(self.a + np.exp(l_m))) / g1l\n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "class gaussian_mean_field_family_lambda_param_overdisp:\n",
    "    \"\"\"Class for a mean field overdispersed gaussian family of arbitrary dimension with scale parametrization \n",
    "    \n",
    "    lambda = log(exp(sigma) - a)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, param, dim, tau):\n",
    "        \"\"\"The parameters for the Gaussian mean field family aare passed as a dictionary\n",
    "        \n",
    "        Args:\n",
    "            param: A dictionaray containing two entries with arrays of dimension (dim,)\n",
    "                 - param[\"mu\"] is a 1-dim Ndarray with the means of independent Gaussian distributions\n",
    "                 - param[\"sigma\"] is a 1-dim Ndarray with the np.log(SDs) of independent Gaussian distributions\n",
    "                 - param[\"a\"] see parametrization \n",
    "            dim: Dimension of the family\n",
    "            tau: Dispersion coefficient >= 1\n",
    "        \"\"\"\n",
    "        \n",
    "        self.mu = param[\"mu\"]\n",
    "        self.sigma = param[\"sigma\"]\n",
    "        self.a = param[\"a\"]\n",
    "        self.tau = tau\n",
    "        self.dim = dim\n",
    "        \n",
    "    def sample(self, n_size):\n",
    "        \"\"\"Samples from the gaussian mean_field variational family\n",
    "        \n",
    "        Args:\n",
    "            n_size: the number of samples to be generated\n",
    "        Returns:\n",
    "            samle: 2 dimensional NDarray where each row is a single sample from the variational family\n",
    "        \"\"\"\n",
    "        \n",
    "        sample = np.zeros((n_size, self.dim))\n",
    "        for i in range(self.dim):\n",
    "            sd = np.log(np.exp(self.sigma[i]) + self.a) * np.sqrt(self.tau[i])\n",
    "            sample[:, i] = np.random.normal(loc = self.mu[i], scale = sd , size = n_size)     \n",
    "        return sample\n",
    "    \n",
    "    def log_pdf(self, theta):\n",
    "        \"\"\"Calculates the value of log density of the mean field gaussian family at theta\n",
    "        \n",
    "        Args:\n",
    "            theta: 1-dim Ndarray of theta values at which the log density will be evaluated\n",
    "        Returns:\n",
    "            log_dens: The value of log density at theta\n",
    "        \"\"\"\n",
    "        \n",
    "        log_dens = 0\n",
    "        for i in range(self.dim):\n",
    "            sd = np.log(np.exp(self.sigma[i]) + self.a) * np.sqrt(self.tau[i])\n",
    "            density = sp.stats.norm(loc = self.mu[i], scale = sd)\n",
    "            log_dens = log_dens + np.log(density.pdf(theta[i]))       \n",
    "        return log_dens\n",
    "    \n",
    "    def grad_log_pdf_tau(self, theta):\n",
    "        \"\"\"\"Calculate the value of score function at theta w.r.t dispersion coefficient tau\n",
    "        \n",
    "        Args:\n",
    "            theta: 1-dim Ndarray of theta values at which the log gradient will be evaluated\n",
    "        Returns:\n",
    "            gradient: 1-dim Ndarray of len(dim) with the values of score function\n",
    "                    w.r.t tau\n",
    "        \"\"\"\n",
    "        gradient = np.zeros(self.dim)\n",
    "        for i in range(self.dim):\n",
    "            sd = np.log(np.exp(self.sigma[i]) + self.a)\n",
    "            gradient[i] = - 1/(2 * self.tau[i]) + (((theta[i] - self.mu[i]) / (self.tau[i] * sd)) ** 2) / 2\n",
    "            \n",
    "        return gradient\n",
    "\n",
    "class gamma_mean_field_family_lambda_param_overdisp:\n",
    "    \"\"\"Class for a mean field overdispersed gamma family of arbitrary, parametrized with mean and standard deviation\n",
    "    \n",
    "        lambda_alfa = log(exp(alfa) - a)\n",
    "        lambda_beta = log(exp(alfa) - a)\n",
    "    \n",
    "        where alpha corresponds to the mean of the gamma distribution and beta corresponds to the std of the gamma dist\n",
    "      \n",
    "      Gamma parametrization:\n",
    "          \n",
    "          f(x|alpha, beta) = \\frac{beta ^ alpha}{Gamma(alpha)} * x ^{alpha - 1} * \\exp{- beta * x}\n",
    "          \n",
    "          E(X) = alpha / beta\n",
    "          Var(X) = alpha / (beta^2)\n",
    "          \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, param, dim, tau):\n",
    "        \"\"\"The parameters for the gamma mean field family are passed as a dictionary\n",
    "        \n",
    "        Args:\n",
    "            param: A dictionaray containing two entries with arrays of dimension (dim,)\n",
    "                 - param[\"alpha\"] is a 1-dim Ndarray with the np.log(alpha) parameters of independent Gamma distributions\n",
    "                 - param[\"beta\"] is a 1-dim Ndarray with the np.log(beta) parameters of independent Gamma distributions\n",
    "                 - param[\"a\"] see above\n",
    "            tau: Overdispersion parameter value\n",
    "        \"\"\"\n",
    "        \n",
    "        self.alpha = param[\"alpha\"]\n",
    "        self.beta = param[\"beta\"]\n",
    "        self.a = param[\"a\"]\n",
    "        self.tau = tau\n",
    "        self.dim = dim\n",
    "        \n",
    "    def sample(self, n_size):\n",
    "        \"\"\"Samples from the gamma mean field variational family\n",
    "        \n",
    "        Note: np.random.gamma has parametrization with shape = alpha, scale = 1 / beta\n",
    "        \n",
    "        Args:\n",
    "            n_size: the number of samples to be generated\n",
    "        Returns:\n",
    "            samle: 2 dimensional NDarray where each row is a single sample from the variational family\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        sample = np.zeros((n_size, self.dim))\n",
    "        for i in range(self.dim):\n",
    "            mu = np.log(np.exp(self.alpha[i]) + self.a)\n",
    "            sigma = np.log(np.exp(self.beta[i]) + self.a)\n",
    "            alpha = (float(mu/sigma) ** 2 + self.tau[i] - 1) / self.tau[i]\n",
    "            beta = float(mu/ (sigma**2)) / self.tau[i]\n",
    "            tfd = tfp.distributions\n",
    "            tfdGamma = tfd.Gamma(concentration=alpha, rate=beta)\n",
    "            samples = tfdGamma.sample(n_size)\n",
    "            sample[:, i] = samples.numpy()\n",
    "        return sample\n",
    "    \n",
    "    def log_pdf(self, theta):\n",
    "        \"\"\"Calculates the value of log density of the mean field gamma family at theta\n",
    "        \n",
    "        Args:\n",
    "            theta: 1-dim Ndarray of theta values at which the log density will be evaluated\n",
    "        Returns:\n",
    "            log_dens: The value of log density at theta\n",
    "        \"\"\"\n",
    "        \n",
    "        log_dens = 0\n",
    "        for i in range(self.dim):\n",
    "            \n",
    "            mu = np.log(np.exp(self.alpha[i]) + self.a)\n",
    "            sigma = np.log(np.exp(self.beta[i]) + self.a)\n",
    "            alpha = (float(mu/sigma) ** 2 + self.tau[i] - 1) / self.tau[i]\n",
    "            beta = float(mu/ (sigma**2)) / self.tau[i]\n",
    "            tfd = tfp.distributions\n",
    "            tfdGamma = tfd.Gamma(concentration=alpha, rate=beta)\n",
    "            log_dens = log_dens + float(tfdGamma.log_prob(theta[i]))\n",
    "        return log_dens\n",
    "    \n",
    "    \n",
    "    def grad_log_pdf_tau(self, theta):\n",
    "        \"\"\"\"Calculate the value of score function at theta w.r.t dispersion coefficient tau\n",
    "        \n",
    "        Args:\n",
    "            theta: 1-dim Ndarray of theta values at which the log gradient will be evaluated\n",
    "        Returns:\n",
    "            gradient: 1-dim Ndarray of len(dim) with the values of score function\n",
    "                    w.r.t tau\n",
    "        \"\"\"\n",
    "        \n",
    "        gradient = np.zeros(self.dim)\n",
    "        for i in range(self.dim):    \n",
    "            mu = np.log(np.exp(self.alpha[i]) + self.a)\n",
    "            sigma = np.log(np.exp(self.beta[i]) + self.a)\n",
    "            alpha = (float(mu/sigma) ** 2)\n",
    "            beta = float(mu/ (sigma**2))\n",
    "            \n",
    "            gradient[i] = (self.tau[i] * np.log(beta / self.tau[i]) - (alpha + self.tau[i] - 1) + beta * theta[i] - \\\n",
    "                          np.log(beta / self.tau[i]) * (alpha + self.tau[i] - 1) + sp.special.digamma((alpha + self.tau[i] - 1) /self.tau[i]) * (alpha - 1) - \\\n",
    "                          np.log(theta[i]) * (alpha - 1)) / self.tau[i]      \n",
    "        return gradient\n",
    "##### END of distribution class definition\n",
    "\n",
    "\n",
    "##### GP model means class definintions    \n",
    "class delta_mean:\n",
    "    def __init__(self, hyperparametrs):\n",
    "        \"\"\"General class representing the mean function of model discrepancy GP\n",
    "        \n",
    "        Args:\n",
    "            hyperparameters: A dictionary of hyperparameters for the mean\n",
    "        \"\"\"\n",
    "        self.hyperparameters = hyperparametrs\n",
    "        \n",
    "    def zero(self, x):\n",
    "        \"\"\"Zero mean: m(x) = 0\n",
    "        \n",
    "        Args:\n",
    "            x: 1-dim Ndarray of model imputs (or scalar)\n",
    "        Returns: 0\n",
    "        \"\"\"\n",
    "        return 0\n",
    "    \n",
    "    def constant(self, x):\n",
    "        \"\"\"Constant mean: m(x) = \\Beta\n",
    "        \n",
    "        Args:\n",
    "            x: 1-dim Ndarray of model imputs (or scalar)\n",
    "            self.hyperparameters: A scalar with dictionary key \"Beta\"\n",
    "        Returns: self.hyperparameters[\"Beta\"]\n",
    "        \"\"\"\n",
    "        return self.hyperparameters[\"Beta\"]\n",
    "    \n",
    "    def linear(self, x):\n",
    "        \"\"\"Linear mean: m(x) =Intercept + \\Beta * x^T\n",
    "        \n",
    "        Args:\n",
    "            x: 1-dim Ndarray of model imputs (or scalar)\n",
    "            self.hyperparameters: A scalar with dictionary key \"Intercept\" and a vector of parameters\n",
    "                                  with dictionary key \"Beta\" of the same length as lengt(x)\n",
    "        Returns:\n",
    "            mean: Intercept + \\Beta * (x, theta)^T\n",
    "        \"\"\"\n",
    "        mean = self.hyperparameters[\"Intercept\"] + np.dot(self.hyperparameters[\"Beta\"], x)\n",
    "        return mean\n",
    "    \n",
    "    \n",
    "class model_mean:\n",
    "    def __init__(self, hyperparametrs):\n",
    "        \"\"\"General class representing the mean function of computer model GP\n",
    "        \n",
    "        Args:\n",
    "            hyperparameters: A dictionary of hyperparameters for the mean\n",
    "        \"\"\"\n",
    "        self.hyperparameters = hyperparametrs\n",
    "        \n",
    "    def zero(self, x, theta):\n",
    "        \"\"\"Zero mean: m(x,theta) = 0\n",
    "        \n",
    "        Args:\n",
    "            x: 1-dim Ndarray of model imputs (or scalar)\n",
    "            theta: 1-dim Ndarray (or scalar) of calibration parameters  \n",
    "        Returns: 0\n",
    "        \"\"\"\n",
    "        return 0\n",
    "    \n",
    "    def constant(self, x, theta):\n",
    "        \"\"\"Constant mean: m(x,theta) = \\Beta\n",
    "        \n",
    "        Args:\n",
    "            x: 1-dim Ndarray of model imputs (or scalar)\n",
    "            theta: 1-dim Ndarray (or scalar) of calibration parameters\n",
    "            self.hyperparameters: A scalar with dictionary key \"Beta\"\n",
    "        Returns: self.hyperparameters[\"Beta\"]\n",
    "        \"\"\"\n",
    "        return self.hyperparameters[\"Beta\"]\n",
    "    \n",
    "    def dot_product(self, x, theta):\n",
    "        \"\"\"Dot product mean, for two dimensional model input x as defined in the \n",
    "           simulation setup in Kejzlar and Maiti (2020): \n",
    "           \n",
    "           m(x,theta) = \\Beta * (\\theta_1 * cos(x_1) + \\theta_2 * cos(x_2))\n",
    "        \n",
    "        Args:\n",
    "            x: 1-dim Ndarray of model imputs (or scalar)\n",
    "            theta: 1-dim Ndarray (or scalar) of calibration parameter\n",
    "            self.hyperparameters: A scalar with dictionary key \"Beta\"\n",
    "        Returns: self.hyperparameters[\"Beta\"] * np.dot(x, theta)\n",
    "        \"\"\"\n",
    "        if x.ndim == 1:\n",
    "            return self.hyperparameters[\"Beta\"] * np.dot(np.array([np.cos(x[0]), np.sin(x[1])]), theta)\n",
    "        else:\n",
    "            return self.hyperparameters[\"Beta\"] * np.dot(np.array([np.cos(x[:,0]), np.sin(x[:,1])]), theta)\n",
    "##### END of GP model means class definitnon\n",
    "    \n",
    "##### Adaptive learning rates definitnion AdaGrad, Adam, RMSProp    \n",
    "def AdaGrad(diag_G, gradient, eta = 0.01, eps = 1 / (10 ** 8)):\n",
    "    \"\"\"Adaptive Learning rate computation using AdaGrad.\n",
    "    \n",
    "    Args:\n",
    "        diag_G: Diagonal of the sum of squared gradients from previous steps\n",
    "        gradient: The value of the gradiant in the current step of SGA\n",
    "        eta: Innitial step size\n",
    "        eps: Small constant to prevent division by 0\n",
    "        \n",
    "    Returns:\n",
    "        rho: Step size for SGA\n",
    "        diag_G_next: Diagonal of the sum of squared gradients from previous steps + gradient ** 2 \n",
    "    \"\"\"\n",
    "    \n",
    "    diag_G_next = diag_G + gradient ** 2\n",
    "    rho = eta / np.sqrt(eps + diag_G_next)   \n",
    "    return rho, diag_G_next\n",
    "\n",
    "def RMSProp(avg_old, gradient, decay = 1 / 2, eta = 0.01 , eps = 1 / (10 ** 6)):\n",
    "    \"\"\"Adaptive learning rate computation using RMSProp.\n",
    "    \n",
    "    Args:\n",
    "        avg_old: The value decay * avg_old + (1 - decay) * gradient ** 2 from previous step\n",
    "        gradient: The value of the gradiant in the current step of SGA\n",
    "        decay: The value of forgetting factor\n",
    "        eta: Innitial step size\n",
    "        eps: Small constant to prevent division by 0\n",
    "        \n",
    "    Returns:\n",
    "        rho: Step size for SGA\n",
    "        avg_next: The value of decay * avg_old + (1 - decay) * gradient ** 2 for the currents step\n",
    "    \"\"\"\n",
    "    avg_new =  decay * avg_old + (1 - decay) * gradient ** 2\n",
    "    rho =  eta / np.sqrt(avg_new + eps)\n",
    "    return rho, avg_new\n",
    "\n",
    "def Adam(m_old, v_old, gradient, step_counter, decay = 1 / 2, eta = 0.01 , eps = 1 / (10 ** 6)):\n",
    "    \"\"\"Calculates the proposed change in the value of parameters that are being optimized with SGA using\n",
    "       the Adam algorithim, i.e. computes the value of \"proposal\", where param_new = param_old + proposal\n",
    "    \n",
    "    Args:\n",
    "        m_old: The value decay * m_old + (1 - decay) * gradient from previous step\n",
    "        v_old: The value decay * m_old + (1 - decay) * gradient ** 2 from previous step\n",
    "        gradient: The value of the gradiant in the current step of SGA\n",
    "        step_counter: The current step value in the SGA\n",
    "        decay: The value of forgetting factor\n",
    "        eta: Innitial step size\n",
    "        eps: Small constant to prevent division by 0\n",
    "        \n",
    "    Returns:\n",
    "        proposal: Teh proposed \"addition\" to the parameters value in SGA\n",
    "        m_old: The value decay * m_old + (1 - decay) * gradient from the current step\n",
    "        v_old: The value decay * m_old + (1 - decay) * gradient ** 2 from the current step\n",
    "    \"\"\"\n",
    "    \n",
    "    m_new = decay * m_old + (1 - decay) * gradient\n",
    "    v_new = decay * v_old + (1 - decay) * gradient ** 2\n",
    "    m_new_hat = m_new / (1 - decay ** step_counter)\n",
    "    v_new_hat = v_new / (1 - decay ** step_counter)\n",
    "    proposal = eta * m_new_hat / np.sqrt(v_new_hat + eps)\n",
    "    return proposal, m_new, v_new\n",
    "##### END of adaptive learning rates defitnitons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Variational Calibration - Rao-Blackwellization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##### This chunk containts the main interface functions for VC with Rao-Blackwellization only\n",
    "##### however, it also has some utilities for other variance reduction functions, so run this code\n",
    "##### irrespective the version of VC\n",
    "\n",
    "def variational_doctioinary_to_lambda(variational_dictionary):\n",
    "    \"\"\"Function takes a dictionary of variational mean field family and transforms it into a 1-dim Ndarray\n",
    "    of that represents a gradient of log pdfs. The oredr in the gradient is as follows:\n",
    "    [theat.mu, theta.sigma, sigma.alpha, sigma.beta, kernel_f_l.alpha, kernel_f_l.beta,\n",
    "    kernel_f_eta.alpha, kernel_f_eta.beta, kernel_delta_l.alpha, kernel_delta_l.beta, kernel_delta_eta.alpha,\n",
    "    kernel_delta_eta.beta] and then [mean_f.mu, mean_f.sigma, mean_delta.mu, mean_delta.sigma] if the mean \n",
    "    hyperparameters are not fixed.\n",
    "    \n",
    "    Args:\n",
    "        variational_dictionary: A dictionary of instances of gamma_mean_field_family and gaussian_mean_field_family\n",
    "                            in a dictionary with keys {\"theta\", \"sigma\", \"kernel_f_l\", \"kernel_f_eta\",\n",
    "                            \"kernel_delta_l\", \"kernel_delta_eta\"} and \"mean_f\" or \"mean_delta\" if the\n",
    "                            mean hyperparameters are not fixed.\n",
    "    Return:\n",
    "        log_variational_grad: A 1-dim NDarray of of gradient of log variational pdf where the gradients are in\n",
    "                            the order: theta, sigma, kernel_f_l, kernel_f_eta, kernel_delta_l, kernel_delta_eta,\n",
    "                            mean_f, mean_delta\n",
    "        indexes: A list of strings that outlines what value in log_variational_grad corresponds to which part \n",
    "                of the GP specifications.\n",
    "        \n",
    "    \"\"\"\n",
    "    indexes = []\n",
    "    ### Theta + kernels\n",
    "    var_lambda = variational_dictionary[\"theta\"].mu\n",
    "    if \"kernel_f_l\" in variational_dictionary:\n",
    "        var_lambda = np.concatenate([var_lambda, variational_dictionary[\"theta\"].sigma,\n",
    "                                     variational_dictionary[\"sigma\"].alpha,\n",
    "                                     variational_dictionary[\"sigma\"].beta,\n",
    "                                     variational_dictionary[\"kernel_f_l\"].alpha,\n",
    "                                     variational_dictionary[\"kernel_f_l\"].beta,\n",
    "                                     variational_dictionary[\"kernel_f_eta\"].alpha,\n",
    "                                     variational_dictionary[\"kernel_f_eta\"].beta,\n",
    "                                     variational_dictionary[\"kernel_delta_l\"].alpha,\n",
    "                                     variational_dictionary[\"kernel_delta_l\"].beta,\n",
    "                                     variational_dictionary[\"kernel_delta_eta\"].alpha,\n",
    "                                     variational_dictionary[\"kernel_delta_eta\"].beta])\n",
    "        \n",
    "        indexes = indexes + [\"theta\"] * variational_dictionary[\"theta\"].dim * 2 + \\\n",
    "                        [\"sigma\"] * variational_dictionary[\"sigma\"].dim * 2 + \\\n",
    "                        [\"kernel_f_l\"] * variational_dictionary[\"kernel_f_l\"].dim * 2 + \\\n",
    "                        [\"kernel_f_eta\"] * variational_dictionary[\"kernel_f_eta\"].dim * 2 + \\\n",
    "                        [\"kernel_delta_l\"] * variational_dictionary[\"kernel_delta_l\"].dim * 2 + \\\n",
    "                        [\"kernel_delta_eta\"] * variational_dictionary[\"kernel_delta_eta\"].dim * 2\n",
    "    else:\n",
    "        var_lambda = np.concatenate([var_lambda, variational_dictionary[\"theta\"].sigma,\n",
    "                                     variational_dictionary[\"sigma\"].alpha,\n",
    "                                     variational_dictionary[\"sigma\"].beta,\n",
    "                                     variational_dictionary[\"kernel_delta_l\"].alpha,\n",
    "                                     variational_dictionary[\"kernel_delta_l\"].beta,\n",
    "                                     variational_dictionary[\"kernel_delta_eta\"].alpha,\n",
    "                                     variational_dictionary[\"kernel_delta_eta\"].beta])\n",
    "        indexes = indexes + [\"theta\"] * variational_dictionary[\"theta\"].dim * 2 + \\\n",
    "                        [\"sigma\"] * variational_dictionary[\"sigma\"].dim * 2 + \\\n",
    "                        [\"kernel_delta_l\"] * variational_dictionary[\"kernel_delta_l\"].dim * 2 + \\\n",
    "                        [\"kernel_delta_eta\"] * variational_dictionary[\"kernel_delta_eta\"].dim * 2\n",
    "        \n",
    "\n",
    "    ### Means\n",
    "    if \"mean_f\" in variational_dictionary:\n",
    "        var_lambda = np.concatenate([var_lambda, variational_dictionary[\"mean_f\"].mu,\n",
    "                                         variational_dictionary[\"mean_f\"].sigma])   \n",
    "    if \"mean_delta\" in variational_dictionary:\n",
    "        var_lambda = np.concatenate([var_lambda,  variational_dictionary[\"mean_delta\"].mu,\n",
    "                                         variational_dictionary[\"mean_delta\"].sigma])\n",
    "    ## Indexes  \n",
    "\n",
    "\n",
    "    if \"mean_f\" in variational_dictionary:\n",
    "        indexes = indexes + [\"mean_f\"] * variational_dictionary[\"mean_f\"].dim * 2\n",
    "    if \"mean_delta\" in variational_dictionary:\n",
    "        indexes = indexes + [\"mean_delta\"] * variational_dictionary[\"mean_delta\"].dim * 2\n",
    "    \n",
    "    return var_lambda.flatten(), indexes\n",
    "\n",
    "def ELBO_mask_SGA(i_low_type, i_hig_type, ELBO, variational_dictionary):\n",
    "    \"\"\"Function calculates the mask of ELBO so that the noisy ELBO gradient estimates are unbiased,\n",
    "       based on the type of argument passed onto the function, i.e. sets ELBO = 0 for appropriate \n",
    "       elements of the elbo, for example if i_low_type = i_hig_type = 0, the elements of \n",
    "       delta GP are not updated\n",
    "    \n",
    "    Args:\n",
    "        i_low_type: 1 for y, 0 for z\n",
    "        i_hig_type: 1 for y, 0 for z\n",
    "        ELBO: An array corresponding to the ELBO\n",
    "        variational_dictionary: A dictionary of instances of gamma_mean_field_family and gaussian_mean_field_family\n",
    "                            in a dictionary with keys {\"theta\", \"sigma\", \"kernel_f_l\", \"kernel_f_eta\",\n",
    "                            \"kernel_delta_l\", \"kernel_delta_eta\"} and \"mean_f\" or \"mean_delta\" if the\n",
    "                            mean hyperparameters are not fixed.                      \n",
    "    Return:\n",
    "        ELBO_new: Adjusted ELBO\n",
    "    \"\"\"\n",
    "    \n",
    "    indexes = []\n",
    "    \n",
    "    ## Indexes  \n",
    "        \n",
    "    if (i_low_type == 1) & (i_hig_type == 0):\n",
    "        indexes = indexes + [1] * variational_dictionary[\"theta\"].dim * 2 + \\\n",
    "                        [1] * variational_dictionary[\"sigma\"].dim * 2 + \\\n",
    "                        [1] * variational_dictionary[\"kernel_f_l\"].dim * 2 + \\\n",
    "                        [1] * variational_dictionary[\"kernel_f_eta\"].dim * 2 + \\\n",
    "                        [1] * variational_dictionary[\"kernel_delta_l\"].dim * 2 + \\\n",
    "                        [1] * variational_dictionary[\"kernel_delta_eta\"].dim * 2\n",
    "\n",
    "        if \"mean_f\" in variational_dictionary:\n",
    "            indexes = indexes + [1] * variational_dictionary[\"mean_f\"].dim * 2\n",
    "        if \"mean_delta\" in variational_dictionary:\n",
    "            indexes = indexes + [1] * variational_dictionary[\"mean_delta\"].dim * 2\n",
    "\n",
    "    elif (i_low_type == 0) & (i_hig_type == 1):   \n",
    "        indexes = indexes + [1] * variational_dictionary[\"theta\"].dim * 2 + \\\n",
    "                        [1] * variational_dictionary[\"sigma\"].dim * 2 + \\\n",
    "                        [1] * variational_dictionary[\"kernel_f_l\"].dim * 2 + \\\n",
    "                        [1] * variational_dictionary[\"kernel_f_eta\"].dim * 2 + \\\n",
    "                        [1] * variational_dictionary[\"kernel_delta_l\"].dim * 2 + \\\n",
    "                        [1] * variational_dictionary[\"kernel_delta_eta\"].dim * 2\n",
    "\n",
    "        if \"mean_f\" in variational_dictionary:\n",
    "            indexes = indexes + [1] * variational_dictionary[\"mean_f\"].dim * 2\n",
    "        if \"mean_delta\" in variational_dictionary:\n",
    "            indexes = indexes + [1] * variational_dictionary[\"mean_delta\"].dim * 2\n",
    "\n",
    "\n",
    "    elif (i_low_type == 0) & (i_hig_type == 0):\n",
    "        indexes = indexes + [0] * variational_dictionary[\"theta\"].dim * 2 + \\\n",
    "                        [0] * variational_dictionary[\"sigma\"].dim * 2 + \\\n",
    "                        [1] * variational_dictionary[\"kernel_f_l\"].dim * 2 + \\\n",
    "                        [1] * variational_dictionary[\"kernel_f_eta\"].dim * 2 + \\\n",
    "                        [0] * variational_dictionary[\"kernel_delta_l\"].dim * 2 + \\\n",
    "                        [0] * variational_dictionary[\"kernel_delta_eta\"].dim * 2\n",
    "\n",
    "        if \"mean_f\" in variational_dictionary:\n",
    "            indexes = indexes + [1] * variational_dictionary[\"mean_f\"].dim * 2\n",
    "        if \"mean_delta\" in variational_dictionary:\n",
    "            indexes = indexes + [0] * variational_dictionary[\"mean_delta\"].dim * 2\n",
    "\n",
    "\n",
    "    elif (i_low_type == 1) & (i_hig_type == 1):\n",
    "        \n",
    "        if \"kernel_f_l\" in variational_dictionary:\n",
    "            indexes = indexes + [1] * variational_dictionary[\"theta\"].dim * 2 + \\\n",
    "                            [1] * variational_dictionary[\"sigma\"].dim * 2 + \\\n",
    "                            [1] * variational_dictionary[\"kernel_f_l\"].dim * 2 + \\\n",
    "                            [1] * variational_dictionary[\"kernel_f_eta\"].dim * 2 + \\\n",
    "                            [1] * variational_dictionary[\"kernel_delta_l\"].dim * 2 + \\\n",
    "                            [1] * variational_dictionary[\"kernel_delta_eta\"].dim * 2\n",
    "        else:\n",
    "            indexes = indexes + [1] * variational_dictionary[\"theta\"].dim * 2 + \\\n",
    "                            [1] * variational_dictionary[\"sigma\"].dim * 2 + \\\n",
    "                            [1] * variational_dictionary[\"kernel_delta_l\"].dim * 2 + \\\n",
    "                            [1] * variational_dictionary[\"kernel_delta_eta\"].dim * 2\n",
    "            \n",
    "\n",
    "        if \"mean_f\" in variational_dictionary:\n",
    "            indexes = indexes + [1] * variational_dictionary[\"mean_f\"].dim * 2\n",
    "        if \"mean_delta\" in variational_dictionary:\n",
    "            indexes = indexes + [1] * variational_dictionary[\"mean_delta\"].dim * 2\n",
    "            \n",
    "    ELBO_new = ELBO * np.array(indexes)\n",
    "    return ELBO_new\n",
    "\n",
    "\n",
    "def var_lambda_to_variational_dictionary(variational_dictionary, var_lambda, indexes):\n",
    "    \"\"\"Function takes the current the the updated value of variational parameters from the\n",
    "    upcoming step in the SGA and updates the variational dictionary with these values.\n",
    "    \n",
    "    Args:\n",
    "        variational_dictionary: A dictionary of instances of gamma_mean_field_family and gaussian_mean_field_family\n",
    "                            in a dictionary with keys {\"theta\", \"sigma\", \"kernel_f_l\", \"kernel_f_eta\",\n",
    "                            \"kernel_delta_l\", \"kernel_delta_eta\"} and \"mean_f\" or \"mean_delta\" if the\n",
    "                            mean hyperparameters are not fixed.\n",
    "        var_lambda: A 1-dim NDarray of updated values of variational parameters in order [theat.mu, theta.sigma,\n",
    "                    sigma.alpha, sigma.beta, kernel_f_l.alpha, kernel_f_l.beta,\n",
    "                    kernel_f_eta.alpha, kernel_f_eta.beta, kernel_delta_l.alpha, kernel_delta_l.beta, kernel_delta_eta.alpha,\n",
    "                    kernel_delta_eta.beta] and then [mean_f.mu, mean_f.sigma, mean_delta.mu, mean_delta.sigma] if the mean \n",
    "                    hyperparameters are not fixed.\n",
    "        indexes: A list of strings that outlines what value in var_lambda corresponds to which part \n",
    "                of the GP specifications.\n",
    "    Return:\n",
    "        variational_dictionary: Updated variational dictionary\n",
    "    \"\"\"\n",
    "    var_lambda_pd = pd.DataFrame(var_lambda.reshape(1, len(indexes)), columns=indexes)\n",
    "\n",
    "    # Theta\n",
    "    dim = variational_dictionary[\"theta\"].dim\n",
    "    variational_dictionary[\"theta\"].mu = np.array(var_lambda_pd[\"theta\"])[0,:dim].astype(float)\n",
    "    variational_dictionary[\"theta\"].sigma = np.array(var_lambda_pd[\"theta\"])[0,dim:].astype(float)\n",
    "    # Noise\n",
    "    variational_dictionary[\"sigma\"].alpha = np.array(var_lambda_pd[\"sigma\"])[0,:1].astype(float)\n",
    "    variational_dictionary[\"sigma\"].beta = np.array(var_lambda_pd[\"sigma\"])[0,1:].astype(float)\n",
    "    # kernel_f\n",
    "    if \"kernel_f_l\" in variational_dictionary:\n",
    "        dim = variational_dictionary[\"kernel_f_l\"].dim\n",
    "        variational_dictionary[\"kernel_f_l\"].alpha = np.array(var_lambda_pd[\"kernel_f_l\"])[0, :dim].astype(float)\n",
    "        variational_dictionary[\"kernel_f_l\"].beta = np.array(var_lambda_pd[\"kernel_f_l\"])[0, dim:].astype(float)\n",
    "\n",
    "        variational_dictionary[\"kernel_f_eta\"].alpha = np.array(var_lambda_pd[\"kernel_f_eta\"])[0,:1].astype(float)\n",
    "        variational_dictionary[\"kernel_f_eta\"].beta = np.array(var_lambda_pd[\"kernel_f_eta\"])[0,1:].astype(float)\n",
    "    # kernel_delta\n",
    "    dim = variational_dictionary[\"kernel_delta_l\"].dim\n",
    "    variational_dictionary[\"kernel_delta_l\"].alpha = np.array(var_lambda_pd[\"kernel_delta_l\"])[0, :dim].astype(float)\n",
    "    variational_dictionary[\"kernel_delta_l\"].beta = np.array(var_lambda_pd[\"kernel_delta_l\"])[0, dim:].astype(float)\n",
    "\n",
    "    variational_dictionary[\"kernel_delta_eta\"].alpha = np.array(var_lambda_pd[\"kernel_delta_eta\"])[0,:1].astype(float)\n",
    "    variational_dictionary[\"kernel_delta_eta\"].beta = np.array(var_lambda_pd[\"kernel_delta_eta\"])[0,1:].astype(float)\n",
    "\n",
    "    if \"mean_f\" in variational_dictionary:\n",
    "        dim = variational_dictionary[\"mean_f\"].dim\n",
    "        variational_dictionary[\"mean_f\"].mu = np.array(var_lambda_pd[\"mean_f\"])[0,:dim].astype(float)\n",
    "        variational_dictionary[\"mean_f\"].sigma = np.array(var_lambda_pd[\"mean_f\"])[0,dim:].astype(float)\n",
    "    if \"mean_delta\" in variational_dictionary:\n",
    "        dim = variational_dictionary[\"mean_delta\"].dim\n",
    "        variational_dictionary[\"mean_delta\"].mu = np.array(var_lambda_pd[\"mean_delta\"])[0,:dim].astype(float)\n",
    "        variational_dictionary[\"mean_delta\"].sigma = np.array(var_lambda_pd[\"mean_delta\"])[0,dim:].astype(float)\n",
    "    \n",
    "    return variational_dictionary\n",
    "\n",
    "def delta_ELBO_hyper_priors_variational_RB(priors_dictionary, variational_dictionary, theta_sample_dictionary):\n",
    "    \"\"\"Function calculate the values of log_priors_pdf, log_variational_pdf, and log_variational_grad, given\n",
    "       the current value of variational parameters.\n",
    "       \n",
    "    Args:\n",
    "        theta_sample_dictionary: A sample from the variational family as a dictionary containing the keys\n",
    "                                {\"theta\", \"sigma\", \"kernel_f_l\", \"kernel_f_eta\", \"kernel_delta_l\", \"kernel_delta_eta\"}\n",
    "                                and \"mean_f\" or \"mean_delta\" if the mean hyperparameters are not fixed.\n",
    "        priors_dictionary: A dictionary of instances of gamma_mean_field_family and gaussian_mean_field_family\n",
    "                            in a dictionary with keys {\"theta\", \"sigma\", \"kernel_f_l\", \"kernel_f_eta\",\n",
    "                            \"kernel_delta_l\", \"kernel_delta_eta\"} and \"mean_f\" or \"mean_delta\" if the\n",
    "                            mean hyperparameters are not fixed.\n",
    "        variational_dictionary: A dictionary of instances of gamma_mean_field_family and gaussian_mean_field_family\n",
    "                            in a dictionary with keys {\"theta\", \"sigma\", \"kernel_f_l\", \"kernel_f_eta\",\n",
    "                            \"kernel_delta_l\", \"kernel_delta_eta\"} and \"mean_f\" or \"mean_delta\" if the\n",
    "                            mean hyperparameters are not fixed.\n",
    "    Return:\n",
    "        log_priors_pdf: A float scalar of the logarithm of priors pdf for all the parameters.\n",
    "        log_variational_pdf: A float scalar of the logarithm of variational mean field family pdf\n",
    "                            for all the parameters.\n",
    "        log_variational_grad: A 1-dim NDarray of of gradient of log variational pdf where the gradients are in\n",
    "                            the order: theta, sigma, kernel_f_l, kernel_f_eta, kernel_delta_l, kernel_delta_eta,\n",
    "                            mean_f, mean_delta\n",
    "    \"\"\"\n",
    "    log_priors_pdf = np.repeat(priors_dictionary[\"theta\"].log_pdf(theta_sample_dictionary[\"theta\"]), priors_dictionary[\"theta\"].dim * 2)  \n",
    "    log_variational_pdf = np.repeat(variational_dictionary[\"theta\"].log_pdf(theta_sample_dictionary[\"theta\"]), variational_dictionary[\"theta\"].dim * 2)\n",
    "    # Going over all the priors\n",
    "    ### Theta prior\n",
    "    if \"kernel_f_l\" in priors_dictionary:\n",
    "        log_priors_pdf = np.concatenate([log_priors_pdf, np.repeat(priors_dictionary[\"sigma\"].log_pdf(theta_sample_dictionary[\"sigma\"]), priors_dictionary[\"sigma\"].dim * 2), \n",
    "                                         np.repeat(priors_dictionary[\"kernel_f_l\"].log_pdf(theta_sample_dictionary[\"kernel_f_l\"]),priors_dictionary[\"kernel_f_l\"].dim * 2) ,\n",
    "                                         np.repeat(priors_dictionary[\"kernel_f_eta\"].log_pdf(theta_sample_dictionary[\"kernel_f_eta\"]), priors_dictionary[\"kernel_f_eta\"].dim * 2) ,\n",
    "                                         np.repeat(priors_dictionary[\"kernel_delta_l\"].log_pdf(theta_sample_dictionary[\"kernel_delta_l\"]), priors_dictionary[\"kernel_delta_l\"].dim * 2),\n",
    "                                         np.repeat(priors_dictionary[\"kernel_delta_eta\"].log_pdf(theta_sample_dictionary[\"kernel_delta_eta\"]), priors_dictionary[\"kernel_delta_eta\"].dim*2)])\n",
    "    else:\n",
    "        log_priors_pdf = np.concatenate([log_priors_pdf, np.repeat(priors_dictionary[\"sigma\"].log_pdf(theta_sample_dictionary[\"sigma\"]), priors_dictionary[\"sigma\"].dim * 2), \n",
    "                                     np.repeat(priors_dictionary[\"kernel_delta_l\"].log_pdf(theta_sample_dictionary[\"kernel_delta_l\"]), priors_dictionary[\"kernel_delta_l\"].dim * 2),\n",
    "                                     np.repeat(priors_dictionary[\"kernel_delta_eta\"].log_pdf(theta_sample_dictionary[\"kernel_delta_eta\"]), priors_dictionary[\"kernel_delta_eta\"].dim*2)])\n",
    "    ### Means\n",
    "    log_priors_pdf.flatten()\n",
    "    if \"mean_f\" in priors_dictionary:\n",
    "        log_priors_pdf = np.concatenate([log_priors_pdf, np.repeat(priors_dictionary[\"mean_f\"].log_pdf(theta_sample_dictionary[\"mean_f\"]), priors_dictionary[\"mean_f\"].dim * 2)])   \n",
    "    if \"mean_delta\" in priors_dictionary:\n",
    "        log_priors_pdf = np.concatenate([log_priors_pdf, np.repeat(priors_dictionary[\"mean_delta\"].log_pdf(theta_sample_dictionary[\"mean_delta\"]), priors_dictionary[\"mean_delta\"].dim * 2)])   \n",
    " \n",
    "    if \"kernel_f_l\" in priors_dictionary:\n",
    "        log_variational_pdf= np.concatenate([log_variational_pdf, np.repeat(variational_dictionary[\"sigma\"].log_pdf(theta_sample_dictionary[\"sigma\"]), variational_dictionary[\"sigma\"].dim * 2),\n",
    "                                                                np.repeat(variational_dictionary[\"kernel_f_l\"].log_pdf(theta_sample_dictionary[\"kernel_f_l\"]), variational_dictionary[\"kernel_f_l\"].dim * 2),\n",
    "                                                                np.repeat(variational_dictionary[\"kernel_f_eta\"].log_pdf(theta_sample_dictionary[\"kernel_f_eta\"]), variational_dictionary[\"kernel_f_eta\"].dim * 2),\n",
    "                                                                np.repeat(variational_dictionary[\"kernel_delta_l\"].log_pdf(theta_sample_dictionary[\"kernel_delta_l\"]), variational_dictionary[\"kernel_delta_l\"].dim * 2),\n",
    "                                                                np.repeat(variational_dictionary[\"kernel_delta_eta\"].log_pdf(theta_sample_dictionary[\"kernel_delta_eta\"]), variational_dictionary[\"kernel_delta_eta\"].dim * 2)])\n",
    "    else:\n",
    "        log_variational_pdf= np.concatenate([log_variational_pdf, np.repeat(variational_dictionary[\"sigma\"].log_pdf(theta_sample_dictionary[\"sigma\"]), variational_dictionary[\"sigma\"].dim * 2),\n",
    "                                                                np.repeat(variational_dictionary[\"kernel_delta_l\"].log_pdf(theta_sample_dictionary[\"kernel_delta_l\"]), variational_dictionary[\"kernel_delta_l\"].dim * 2),\n",
    "                                                                np.repeat(variational_dictionary[\"kernel_delta_eta\"].log_pdf(theta_sample_dictionary[\"kernel_delta_eta\"]), variational_dictionary[\"kernel_delta_eta\"].dim * 2)])\n",
    "    \n",
    "    ### Means\n",
    "    log_variational_pdf.flatten()\n",
    "    if \"mean_f\" in priors_dictionary:\n",
    "        log_variational_pdf = np.concatenate([log_variational_pdf, np.repeat(variational_dictionary[\"mean_f\"].log_pdf(theta_sample_dictionary[\"mean_f\"]), variational_dictionary[\"mean_f\"].dim*2)])   \n",
    "    if \"mean_delta\" in priors_dictionary:\n",
    "        log_variational_pdf = np.concatenate([log_variational_pdf, np.repeat(variational_dictionary[\"mean_delta\"].log_pdf(theta_sample_dictionary[\"mean_delta\"]), variational_dictionary[\"mean_delta\"].dim*2)])   \n",
    " \n",
    "    ########### Gradient handling log_variational_grad\n",
    "    ### Theta\n",
    "    log_variational_grad = variational_dictionary[\"theta\"].grad_log_pdf(theta_sample_dictionary[\"theta\"]) \n",
    "\n",
    "    ### Noise + other scales\n",
    "    if \"kernel_f_l\" in variational_dictionary:\n",
    "        log_variational_grad = np.concatenate([log_variational_grad, variational_dictionary[\"sigma\"].grad_log_pdf(theta_sample_dictionary[\"sigma\"]),\n",
    "                                                                variational_dictionary[\"kernel_f_l\"].grad_log_pdf(theta_sample_dictionary[\"kernel_f_l\"]),\n",
    "                                                                variational_dictionary[\"kernel_f_eta\"].grad_log_pdf(theta_sample_dictionary[\"kernel_f_eta\"]),\n",
    "                                                                variational_dictionary[\"kernel_delta_l\"].grad_log_pdf(theta_sample_dictionary[\"kernel_delta_l\"]),\n",
    "                                                                variational_dictionary[\"kernel_delta_eta\"].grad_log_pdf(theta_sample_dictionary[\"kernel_delta_eta\"])])\n",
    "    else:\n",
    "        log_variational_grad = np.concatenate([log_variational_grad, variational_dictionary[\"sigma\"].grad_log_pdf(theta_sample_dictionary[\"sigma\"]),\n",
    "                                                                variational_dictionary[\"kernel_delta_l\"].grad_log_pdf(theta_sample_dictionary[\"kernel_delta_l\"]),\n",
    "                                                                variational_dictionary[\"kernel_delta_eta\"].grad_log_pdf(theta_sample_dictionary[\"kernel_delta_eta\"])])\n",
    "    \n",
    "    ### Means\n",
    "    log_variational_grad.flatten()\n",
    "    if \"mean_f\" in priors_dictionary:\n",
    "        log_variational_grad = np.concatenate([log_variational_grad, variational_dictionary[\"mean_f\"].grad_log_pdf(theta_sample_dictionary[\"mean_f\"])])   \n",
    "    if \"mean_delta\" in priors_dictionary:\n",
    "        log_variational_grad = np.concatenate([log_variational_grad, variational_dictionary[\"mean_delta\"].grad_log_pdf(theta_sample_dictionary[\"mean_delta\"])])   \n",
    " \n",
    "    return log_priors_pdf, log_variational_pdf, log_variational_grad\n",
    "\n",
    "def vine_p_RB(i, j, l, n, data_input, kernels_input, means_input, vine_type = \"C\"): #Works for general case\n",
    "    \"\"\"Function calculates the log copula density, and log likelihoods used to compute tilde p\n",
    "    in algorithm 2 (as defined in Kejzlar and Maiti (2020))\n",
    "    \n",
    "    Note:\n",
    "    - Rao-Blackwellizaed Version\n",
    "    - this is the \"p\" for the Algorithm 1 and for Algorithm 1 + Control Variates \n",
    "    - p under Rao-Blackwellization defined below\n",
    "    \n",
    "    Args:\n",
    "        i: Index i from the bijection\n",
    "        j: Index j from the bijection\n",
    "        l: Truncation level\n",
    "        n: Sample size\n",
    "        data_input: A dictionary containing 3 items: Response, X, theta for each of data points\n",
    "        kernels_input: A dictionary containing 3 items: f, delta, and noise kernels\n",
    "        means_input: A dictionary containing 2 items: f, delta\n",
    "        vine_type: \"C\" or \"D\" depending on which wine copule we want to consider\n",
    "        \n",
    "    Returns:\n",
    "        log_c: log bivariate copula density\n",
    "        log_pdf_low: log likelihood of datapoint with smaller index\n",
    "        log_pdf_hig: log likelihood of datapoint with larger index\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if vine_type == \"D\":\n",
    "        if j > l:\n",
    "            c_i_j = 1.0\n",
    "        else:\n",
    "            c_i_j = bivariate_gauss_copula(i, j, data_input, kernels_input, means_input, vine_type = \"D\")\n",
    "\n",
    "        # definition of a_i multiplicatior\n",
    "        a = 2 * l\n",
    "        if i <= l:\n",
    "            a = a - (l + 1 - i)\n",
    "        if i > (n - l):\n",
    "            a = a - (l - n + i)\n",
    "\n",
    "        # definition of b_{i+j} multiplicator\n",
    "        b = 2 * l\n",
    "        if (i + j) <= l:\n",
    "            b = b - (l + 1 - j - i)\n",
    "        if (i + j) > (n - l):\n",
    "            b = b - (l - n + j + i)\n",
    "\n",
    "        # Standardized data\n",
    "        Y_low_std, Y_hig_std, var_low, var_hig, i_low_type, i_hig_type  = std_transform(i, j + i, data_input, kernels_input, means_input)\n",
    "    else:\n",
    "        if j > l:\n",
    "            c_i_j = 1.0\n",
    "        else:\n",
    "            c_i_j = bivariate_gauss_copula(i, j, data_input, kernels_input, means_input, vine_type = \"C\")\n",
    "\n",
    "        # definition of a_i multiplier \n",
    "        a = n - 1\n",
    "        # definition of b_{j+i}\n",
    "        b = l\n",
    "        if (j + i) <= l:\n",
    "            b = b + (n - 1 - l)\n",
    "              \n",
    "        # Standardized data\n",
    "        Y_low_std, Y_hig_std, var_low, var_hig, i_low_type, i_hig_type = std_transform(j, j + i, data_input, kernels_input, means_input)\n",
    "        \n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    \n",
    "\n",
    "    log_c = np.log(c_i_j)\n",
    "    log_pdf_low = (1 / a) * np.log(sp.stats.norm.pdf(Y_low_std))\n",
    "    log_pdf_hig = (1 / b) * np.log(sp.stats.norm.pdf(Y_hig_std))\n",
    "\n",
    "    return log_c, log_pdf_low, log_pdf_hig\n",
    "\n",
    "def vine_p_to_p_eval(log_c, log_pdf_low, log_pdf_hig, i_low_type, i_hig_type, priors_dictionary, means_input = None):\n",
    "    \"\"\"Function takes output from vine_p_RB and turns it into the value tilde \"p\" in algorithm 2 (as defined in Kejzlar and Maiti (2020))\n",
    "    \n",
    "    Args:\n",
    "        log_c: log bivariate copula density\n",
    "        log_pdf_low: log likelihood of datapoint with smaller index\n",
    "        log_pdf_hig: log likelihood of datapoint with larger index\n",
    "        i_low_type: 1 for y, 0 for z\n",
    "        i_hig_type: 1 for y, 0 for z\n",
    "        priors_dictionary: A dictionary of instances of gamma_mean_field_family and gaussian_mean_field_family\n",
    "                            in a dictionary with keys {\"theta\", \"sigma\", \"kernel_f_l\", \"kernel_f_eta\",\n",
    "                            \"kernel_delta_l\", \"kernel_delta_eta\"} and \"mean_f\" or \"mean_delta\" if the\n",
    "                            mean hyperparameters are not fixed.\n",
    "        means_input: A dictionary containing 2 items: f, delta\n",
    "        \n",
    "    Returns:\n",
    "        vine_p: Value of tilde \"p\"\n",
    "    \"\"\"\n",
    "    \n",
    "    if (i_low_type == 1) & (i_hig_type == 0):\n",
    "        if means_input != None:\n",
    "            if (means_input[\"f\"].__name__ == \"zero\") or (means_input[\"f\"].__name__ == \"constant\"):\n",
    "                vine_p_theta = log_c\n",
    "            else:\n",
    "                vine_p_theta = log_c + log_pdf_low\n",
    "        vine_p_sigma = log_c + log_pdf_low\n",
    "        vine_p_f_l = log_c\n",
    "        vine_p_f_eta = log_c + log_pdf_hig + log_pdf_low\n",
    "        vine_p_delta_l = log_c\n",
    "        vine_p_delta_eta = log_c + log_pdf_low\n",
    "        \n",
    "        if \"mean_delta\" in variational_dictionary:\n",
    "            vine_p_beta = log_c + log_pdf_low\n",
    "            \n",
    "\n",
    "    elif (i_low_type == 0) & (i_hig_type == 1):\n",
    "        if means_input != None:\n",
    "            if (means_input[\"f\"].__name__ == \"zero\") or (means_input[\"f\"].__name__ == \"constant\"):\n",
    "                 vine_p_theta = log_c\n",
    "            else:\n",
    "                vine_p_theta = log_c + log_pdf_hig\n",
    "        vine_p_sigma = log_c + log_pdf_hig\n",
    "        vine_p_f_l = log_c\n",
    "        vine_p_f_eta = log_c + log_pdf_hig + log_pdf_low\n",
    "        vine_p_delta_l = log_c\n",
    "        vine_p_delta_eta = log_c + log_pdf_hig\n",
    "        \n",
    "        if \"mean_delta\" in variational_dictionary:\n",
    "            vine_p_beta = log_c + log_pdf_hig\n",
    "\n",
    "\n",
    "\n",
    "    elif (i_low_type == 0) & (i_hig_type == 0):\n",
    "        vine_p_theta = 0\n",
    "        vine_p_sigma = 0\n",
    "        vine_p_f_l = log_c\n",
    "        vine_p_f_eta = log_c + log_pdf_hig + log_pdf_low\n",
    "        vine_p_delta_l = 0\n",
    "        vine_p_delta_eta = 0\n",
    "        \n",
    "        if \"mean_delta\" in variational_dictionary:\n",
    "            vine_p_beta = 0\n",
    "\n",
    "\n",
    "    elif (i_low_type == 1) & (i_hig_type == 1):\n",
    "        if means_input != None:\n",
    "            if (means_input[\"f\"].__name__ == \"zero\") or (means_input[\"f\"].__name__ == \"constant\"):\n",
    "                vine_p_theta = log_c\n",
    "            else:\n",
    "                vine_p_theta = log_c + log_pdf_low + log_pdf_hig\n",
    "        vine_p_sigma = log_c + log_pdf_low + log_pdf_hig\n",
    "        if \"kernel_f_l\" in priors_dictionary:\n",
    "            vine_p_f_l = log_c\n",
    "            vine_p_f_eta = log_c + log_pdf_hig + log_pdf_low\n",
    "        vine_p_delta_l = log_c\n",
    "        vine_p_delta_eta = log_c + log_pdf_low + log_pdf_hig\n",
    "        \n",
    "        if \"mean_delta\" in variational_dictionary:\n",
    "            vine_p_beta = log_c + log_pdf_low + log_pdf_hig\n",
    "            \n",
    "    vine_p = np.repeat(vine_p_theta, priors_dictionary[\"theta\"].dim * 2)\n",
    "    \n",
    "    if \"mean_f\" in variational_dictionary:\n",
    "            vine_p_beta_f = log_c + log_pdf_low + log_pdf_hig\n",
    "    \n",
    "    if \"kernel_f_l\" in priors_dictionary:\n",
    "        vine_p = np.concatenate([vine_p, np.repeat(vine_p_sigma, priors_dictionary[\"sigma\"].dim * 2), \n",
    "                                         np.repeat(vine_p_f_l,priors_dictionary[\"kernel_f_l\"].dim * 2) ,\n",
    "                                         np.repeat(vine_p_f_eta, priors_dictionary[\"kernel_f_eta\"].dim * 2) ,\n",
    "                                         np.repeat(vine_p_delta_l, priors_dictionary[\"kernel_delta_l\"].dim * 2),\n",
    "                                         np.repeat(vine_p_delta_eta, priors_dictionary[\"kernel_delta_eta\"].dim*2)])\n",
    "    else:\n",
    "        vine_p = np.concatenate([vine_p, np.repeat(vine_p_sigma, priors_dictionary[\"sigma\"].dim * 2), \n",
    "                                         np.repeat(vine_p_delta_l, priors_dictionary[\"kernel_delta_l\"].dim * 2),\n",
    "                                         np.repeat(vine_p_delta_eta, priors_dictionary[\"kernel_delta_eta\"].dim*2)])\n",
    "    vine_p.flatten()\n",
    "    \n",
    "    if \"mean_f\" in priors_dictionary:\n",
    "        vine_p = np.concatenate([vine_p, np.repeat(vine_p_beta_f, priors_dictionary[\"mean_f\"].dim * 2)])\n",
    "    \n",
    "    if \"mean_delta\" in priors_dictionary:\n",
    "        vine_p = np.concatenate([vine_p, np.repeat(vine_p_beta, priors_dictionary[\"mean_delta\"].dim * 2)])\n",
    "        \n",
    "    return vine_p\n",
    "\n",
    "def delta_ELBO_hyper_RB(data_input, Y_index, Z_index, input_m, means, kernels,\n",
    "               vine_type, theta_sample_dictionary, priors_dictionary, variational_dictionary,\n",
    "               n_obs, x_dim, theta_dim, i, j, l, n):\n",
    "    \"\"\"Function calculates the gradient of ELBO only with Rao-Blackwellization, i.e. withou control variates\n",
    "    or importance sampling. It works for a general variational family because the specifics are abstracted \n",
    "    into level above.\n",
    "    \n",
    "    Args:\n",
    "        data_input: A dictionary containing 3 items: Response, X, theta for each of data points\n",
    "        Y_index: Index of data points in range 1,..., len(Y)\n",
    "        Z_index: Index of data points in range len(Y) + 1, n\n",
    "        input_m: Variable inputs and parameters form model evaluations, each row is (x, theta)\n",
    "        kernels: A dictionary containing 3 items: {\"f\": string describing the type of kernel,\n",
    "                 \"delta\": string describing the type of kernel, \"noise\": current noise value as float}\n",
    "        means: A dictionary containing 2 items: f, delta\n",
    "               {\"f\": string desciribing the type of mean, \"delta\": string describing the type of mean}\n",
    "               note: if there are no priors present for the mean parameters, the dictionary contains instances \n",
    "               of model_mean and delta_mean classes\n",
    "        vine_type: \"C\" or \"D\" depending on which wine copule we want to consider\n",
    "        theta_sample_dictionary: A sample from the variational family as a dictionary containing the keys\n",
    "                                {\"theta\", \"sigma\", \"kernel_f_l\", \"kernel_f_eta\", \"kernel_delta_l\", \"kernel_delta_eta\"}\n",
    "                                and \"mean_f\" or \"mean_delta\" if the mean hyperparameters are not fixed.\n",
    "        priors_dictionary: A dictionary of instances of gamma_mean_field_family and gaussian_mean_field_family\n",
    "                            in a dictionary with keys {\"theta\", \"sigma\", \"kernel_f_l\", \"kernel_f_eta\",\n",
    "                            \"kernel_delta_l\", \"kernel_delta_eta\"} and \"mean_f\" or \"mean_delta\" if the\n",
    "                            mean hyperparameters are not fixed.\n",
    "        variational_dictionary: A dictionary of instances of gamma_mean_field_family and gaussian_mean_field_family\n",
    "                            in a dictionary with keys {\"theta\", \"sigma\", \"kernel_f_l\", \"kernel_f_eta\",\n",
    "                            \"kernel_delta_l\", \"kernel_delta_eta\"} and \"mean_f\" or \"mean_delta\" if the\n",
    "                            mean hyperparameters are not fixed.\n",
    "        n_obs: The number of experimental observations.\n",
    "        x_dim: The dimension of model inputs.\n",
    "        theta_dim: The dimension of calibraiton parameters.\n",
    "        i: Index i from the bijection\n",
    "        j: Index j from the bijection\n",
    "        l: Truncation level\n",
    "        n: Total number of data, i.e. n_obs + n_m\n",
    "        \n",
    "    Returns:\n",
    "        Elbo_add: The value of gradient of ELBO evaluated at current sample from variational family.\n",
    "                  It is in the form of 1 dim Ndarray vector. In case the computation fails, it is a vector\n",
    "                  of np.nan values.\n",
    "        h: The value of log_variational_grad (nothing is done with it here in this function). In case \n",
    "           the computation of gradient of ELBO fails, it is a vector of np.nan values.\n",
    "           - NOTE: this is an artifact here, but used in the CV version\n",
    "    \"\"\"\n",
    "    # How does theta_smaple_dictionary work:\n",
    "    # the key in the dictionary is what part of the GP specification it is related with a list under the key\n",
    "    # if it is any other key than \"theta\", the first item in the list is what kind of type of kernel/mean it is\n",
    "    # the rest is the current value of hyperparameters\n",
    "    \n",
    "    \n",
    "    kernel_type = kernels[\"f\"]\n",
    "    if kernel_type == \"sq_quad\":\n",
    "        kernel_f = float(theta_sample_dictionary[\"kernel_f_eta\"]) ** 2 * skl.RBF(length_scale = theta_sample_dictionary[\"kernel_f_l\"])       \n",
    "    elif kernel_type == \"matern\":\n",
    "        kernel_f = float(theta_sample_dictionary[\"kernel_f_eta\"]) ** 2 * skl.Matern(length_scale = theta_sample_dictionary[\"kernel_f_l\"], nu = 1.5)\n",
    "    elif kernel_type == \"zero\":\n",
    "        kernel_f = 0\n",
    "    # kernel delta\n",
    "    kernel_type = kernels[\"delta\"]\n",
    "    if kernel_type == \"sq_quad\":\n",
    "        kernel_delta = float(theta_sample_dictionary[\"kernel_delta_eta\"]) ** 2 * skl.RBF(length_scale = theta_sample_dictionary[\"kernel_delta_l\"])       \n",
    "    elif kernel_type == \"matern\":\n",
    "        kernel_delta = float(theta_sample_dictionary[\"kernel_delta_eta\"]) ** 2 * skl.Matern(length_scale = theta_sample_dictionary[\"kernel_delta_l\"],\n",
    "                                                               nu = 1.5)\n",
    "    elif kernel_type == \"zero\":\n",
    "        kernel_delta = 0\n",
    "    \n",
    "    noise = theta_sample_dictionary[\"sigma\"][0]\n",
    "    kernels_input = {\"f\":kernel_f, \"delta\": kernel_delta, \"sigma\": float(noise)}\n",
    "    # mean_f\n",
    "    if \"mean_f\" in priors_dictionary: # Is mean fixed or not?\n",
    "        mean_type = means[\"f\"]\n",
    "        if mean_type != \"linear\":\n",
    "            mean_hyperparameters = {\"Beta\": float(theta_sample_dictionary[\"mean_f\"])}\n",
    "            mean_f_init = model_mean(mean_hyperparameters)\n",
    "            \n",
    "            if mean_type == \"constant\":\n",
    "                mean_f = mean_f_init.constant\n",
    "            elif mean_type == \"dot_product\":\n",
    "                mean_f = mean_f_init.dot_product\n",
    "            elif mean_type == \"LDM\":\n",
    "                mean_f = mean_f_init.LDM\n",
    "            \n",
    "        else:\n",
    "            # Linear mean\n",
    "            mean_hyperparameters = {\"Intercep\": float(theta_sample_dictionary[\"mean_f\"][1]),\n",
    "                                    \"Beta\": float(theta_sample_dictionary[\"mean_f\"][2:])}  \n",
    "            mean_f_init = model_mean(mean_hyperparameters)\n",
    "            mean_f = mean_f_init.linear\n",
    "    else:\n",
    "        mean_f = means[\"f\"]\n",
    "        \n",
    "    # mean_delta\n",
    "    if \"mean_delta\" in priors_dictionary: # Is mean fixed or not?\n",
    "        mean_type = means[\"delta\"]\n",
    "        if mean_type != \"linear\":\n",
    "            mean_hyperparameters = {\"Beta\": float(theta_sample_dictionary[\"mean_delta\"])}\n",
    "            mean_delta_init = delta_mean(mean_hyperparameters)\n",
    "            mean_delta = mean_delta_init.constant # No dot product version here            \n",
    "        else:\n",
    "            # Linear mean\n",
    "            mean_hyperparameters = {\"Intercep\": float(theta_sample_dictionary[\"mean_delta\"][1]),\n",
    "                                    \"Beta\": float(theta_sample_dictionary[\"mean_delta\"][2:])}  \n",
    "            mean_delta_init = delta_mean(mean_hyperparameters)\n",
    "            mean_delta = mean_delta_init.linear\n",
    "    else:\n",
    "        mean_delta = means[\"delta\"]\n",
    "        \n",
    "    means_input = {\"f\": mean_f, \"delta\": mean_delta}\n",
    "\n",
    "                   \n",
    "    log_priors_pdf, log_variational_pdf, log_variational_grad = delta_ELBO_hyper_priors_variational_RB(priors_dictionary,\n",
    "                                                                                                    variational_dictionary,\n",
    "                                                                                                    theta_sample_dictionary)  \n",
    "    \n",
    "    theta_variational = np.repeat(theta_sample_dictionary[\"theta\"], n_obs)\n",
    "    # This works for dim of theta > 1\n",
    "    theta_variational = theta_variational.reshape((n_obs, theta_dim), order='F')\n",
    "    \n",
    "    Y_theta = np.concatenate((Y_index, theta_variational), axis = 1)\n",
    "    if np.any(input_m != None):\n",
    "        Z_theta = np.concatenate((Z_index, input_m[:,x_dim:]), axis = 1)\n",
    "        Theta_input = np.concatenate((Y_theta, Z_theta))\n",
    "    else:\n",
    "        Theta_input = Y_theta\n",
    "    \n",
    "    data_input[\"Theta\"] = Theta_input\n",
    "    \n",
    "    log_c, log_pdf_low, log_pdf_hig = vine_p_RB(i, j, l, n, data_input, kernels_input, means_input, vine_type = vine_type)\n",
    "\n",
    "    if vine_type == \"D\":\n",
    "        i_low = i\n",
    "        i_hig = i + j\n",
    "    elif vine_type == \"C\":\n",
    "        i_low = j\n",
    "        i_hig = i + j\n",
    "            \n",
    "    i_low_type = data_input[\"Response\"][i_low - 1,:][1]\n",
    "    i_hig_type = data_input[\"Response\"][i_hig - 1,:][1]\n",
    "    \n",
    "    p_eval = vine_p_to_p_eval(log_c, log_pdf_low, log_pdf_hig, i_low_type, i_hig_type, priors_dictionary, means_input = means_input)\n",
    "    \n",
    "    \n",
    "    if (np.all(np.isnan(p_eval)) != True) & (np.all(np.isfinite(np.array([p_eval, log_priors_pdf, log_variational_pdf]))) == True):    \n",
    "        ELBO_add = int(round(l * (2 * n - (l + 1)) / 2)) * log_variational_grad * p_eval\n",
    "        ELBO_add = ELBO_add - log_variational_grad * (log_variational_pdf - log_priors_pdf)\n",
    "        h = log_variational_grad \n",
    "        if np.isnan(ELBO_add).all() != True:\n",
    "            return ELBO_add, h\n",
    "        else:\n",
    "            return np.repeat(np.nan, len(log_variational_grad)), np.repeat(np.nan, len(log_variational_grad))         \n",
    "    else:\n",
    "        return np.repeat(np.nan, len(log_variational_grad)), np.repeat(np.nan, len(log_variational_grad))\n",
    "    \n",
    "##### The following function is the main intervace for Variational Calibration\n",
    "\n",
    "def VC_calibration_hyper_RB(data_input, Y_index, Z_index, input_m, kernels,\n",
    "                         means, vine_type, priors_dictionary, variational_dictionary, x_dim, theta_dim, n_steps,\n",
    "                         S, l, folder, learning_rate = \"RMSProp\", n_core = 1, eta = 0.01, decay = 1 / 2):\n",
    "    \"\"\"Scalable variational calibration of computer models according to Kejzlar and Maiti (2020).\n",
    "    \n",
    "    - Rao-Blackwellization ONLY\n",
    "    \n",
    "    Args:\n",
    "        data_input: A dictionary containing 3 items: Response, X, theta for each of data points\n",
    "        Y_index: Index of data points in range 1,..., len(Y)\n",
    "        Z_index: Index of data points in range len(Y) + 1, n\n",
    "        input_m: Ndarray with model inputs.\n",
    "        kernels: A dictionary containing 3 items: {\"f\": string describing the type of kernel,\n",
    "                 \"delta\": string describing the type of kernel, \"noise\": current noise value as float}\n",
    "        means: A dictionary containing 2 items: f, delta\n",
    "               {\"f\": string desciribing the type of mean, \"delta\": string describing the type of mean}\n",
    "               note: if there are no priors present for the mean parameters, the dictionary contains instances \n",
    "               of model_mean and delta_mean classes\n",
    "        vine_type: \"C\" or \"D\" depending on which wine copule we want to consider\n",
    "        theta_sample_dictionary: A sample from the variational family as a dictionary containing the keys\n",
    "                                {\"theta\", \"sigma\", \"kernel_f_l\", \"kernel_f_eta\", \"kernel_delta_l\", \"kernel_delta_eta\"}\n",
    "                                and \"mean_f\" or \"mean_delta\" if the mean hyperparameters are not fixed.\n",
    "        priors_dictionary: A dictionary of instances of gamma_mean_field_family and gaussian_mean_field_family\n",
    "                            in a dictionary with keys {\"theta\", \"sigma\", \"kernel_f_l\", \"kernel_f_eta\",\n",
    "                            \"kernel_delta_l\", \"kernel_delta_eta\"} and \"mean_f\" or \"mean_delta\" if the\n",
    "                            mean hyperparameters are not fixed.\n",
    "        variational_dictionary: A dictionary of instances of gamma_mean_field_family and gaussian_mean_field_family\n",
    "                            in a dictionary with keys {\"theta\", \"sigma\", \"kernel_f_l\", \"kernel_f_eta\",\n",
    "                            \"kernel_delta_l\", \"kernel_delta_eta\"} and \"mean_f\" or \"mean_delta\" if the\n",
    "                            mean hyperparameters are not fixed.\n",
    "        x_dim: The dimension of model inputs.\n",
    "        theta_dim: The dimension of calibraiton parameters.\n",
    "        n_steps: The number of evolutions of SGA to be performed.\n",
    "        S: Numer of samples from the variational family to be taken in the MCMC approximation of ELBO\n",
    "        l: Truncation level\n",
    "        folder: The string with path to the folder where the results would be stored\n",
    "                - stores array of variatioanal parameters and time stamps\n",
    "                - saves every 10 iterations - this can be modified at the end of the function def.\n",
    "        learning_rate: The learning reate algorithm to be used in SGA: \"Basic,\" \"AdaGrad,\" \"RMSProp,\" or \"Adam\"\n",
    "        n_core: The number of cores to be used for the parallel evaluation of MCMC integrals\n",
    "        eta: Innitial learning rate\n",
    "        decay: The value of forgetting factor to be used in either \"RMSProp\" or \"Adam\"\n",
    "    Returns:\n",
    "        time_counter: An array of system time of each evolution step of the SGA\n",
    "        var_lambda_array: A Pandas DataFrame of the evolution of variational parameters during the optimization.\n",
    "                          Each row correspond to a single evolution step\n",
    "    \"\"\"\n",
    "    \n",
    "    time_counter = np.array(time.monotonic())\n",
    "    \n",
    "    n_m = input_m.shape[0]\n",
    "    n_obs = data_input[\"Response\"].shape[0] - n_m\n",
    "    n = n_m + n_obs\n",
    "    bijection_domain = int(round(l * (2 * n - (l + 1)) / 2))\n",
    "    \n",
    "    ##### Innitial variational parameters\n",
    "    var_lambda, indexes = variational_doctioinary_to_lambda(variational_dictionary)\n",
    "    \n",
    "\n",
    "    var_lambda_array = pd.DataFrame(var_lambda.reshape(1, len(indexes)), columns=indexes)\n",
    "    # Creating folder to save results\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    else:\n",
    "        shutil.rmtree(folder)           #removes all the subdirectories!\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    # Learning rate preset\n",
    "    if learning_rate == \"RMSProp\":\n",
    "        # RMSProp preset\n",
    "        eps = 1 / (10 ** 6)\n",
    "        avg_old = np.zeros(len(var_lambda))\n",
    "    elif learning_rate == \"AdaGrad\":   \n",
    "        # AdaGrad preset to default values\n",
    "        eps = 1 / (10 ** 6)\n",
    "        diag_G = np.zeros(len(var_lambda))\n",
    "    elif learning_rate == \"Basic\":\n",
    "    # Fixed step size - Do Not reccomend to use\n",
    "        #eps = 0.05\n",
    "        eps = eta\n",
    "        alpha = - 0.75\n",
    "        t = np.round((eps / bijection_domain) ** (1 / alpha)) # step counter\n",
    "        rho = t ** (alpha)\n",
    "    elif learning_rate == \"Adam\":\n",
    "        eps = 1 / (10 ** 6)\n",
    "        m_old = np.zeros(len(var_lambda))\n",
    "        v_old = np.zeros(len(var_lambda))\n",
    "        step_counter = 1\n",
    "    \n",
    "    for steps in tqdm(range(n_steps), desc = \"SGA optimization\"):\n",
    "        \n",
    "        # Bijection\n",
    "        k = np.random.randint(1, bijection_domain + 1)\n",
    "        i, j = bijection_array(k, n, l)\n",
    "\n",
    "        \n",
    "        #### Variational defintion family at the current value of the variational parameter\n",
    "        variational_dictionary = var_lambda_to_variational_dictionary(variational_dictionary, var_lambda, indexes)\n",
    "        \n",
    "        #### Sampling from the variational family for MCMC approximation \n",
    "        theta_sample_dictionary_MCMC = []\n",
    "        for s in range(S):\n",
    "            theta_sample_dictionary = {}\n",
    "            for item in variational_dictionary:\n",
    "                theta_sample_dictionary[item] = variational_dictionary[item].sample(1).flatten()\n",
    "            theta_sample_dictionary_MCMC = theta_sample_dictionary_MCMC + [theta_sample_dictionary]\n",
    "        \n",
    "        ELBO = 0\n",
    "        ELBO_add_array, h_array = zip(*Parallel(n_jobs = n_core)(delayed(delta_ELBO_hyper_RB)(data_input, Y_index,\n",
    "                                                                                           Z_index, input_m, means,\n",
    "                                                                                           kernels, vine_type,\n",
    "                                                                                           theta_sample_dictionary,\n",
    "                                                                                           priors_dictionary,\n",
    "                                                                                           variational_dictionary,\n",
    "                                                                                           n_obs, x_dim, theta_dim, i,\n",
    "                                                                                           j, l, n) for theta_sample_dictionary in theta_sample_dictionary_MCMC))\n",
    "        ELBO_add_array = np.asarray(ELBO_add_array).T\n",
    "        h_array = np.asarray(h_array).T\n",
    "                \n",
    "        nan_mask = ~np.all(np.isnan(h_array), axis = 0)\n",
    "        h_array = h_array[:, nan_mask]\n",
    "        ELBO_add_array = ELBO_add_array[:, nan_mask]\n",
    "        mean_S = h_array.shape[1]\n",
    "\n",
    "        # Estimate of control variete\n",
    "        num = np.sum((h_array - np.mean(h_array, axis= 1)[:,None]) * (ELBO_add_array - np.mean(ELBO_add_array, axis= 1)[:,None]), axis = 1) / (ELBO_add_array.shape[1] - 1)\n",
    "        denom = np.var(h_array, axis= 1)\n",
    "        a = 0\n",
    "        ELBO = (np.sum(ELBO_add_array, axis = 1) - a * np.sum(h_array, axis = 1)) / mean_S\n",
    "        \n",
    "        # Here we will judge the the types of data\n",
    "        ############################################\n",
    "        if vine_type == \"D\":\n",
    "            i_low = i\n",
    "            i_hig = i + j\n",
    "        elif vine_type == \"C\":\n",
    "            i_low = j\n",
    "            i_hig = i + j\n",
    "            \n",
    "        i_low_type = data_input[\"Response\"][i_low - 1,:][1]\n",
    "        i_hig_type = data_input[\"Response\"][i_hig - 1,:][1]\n",
    "        \n",
    "        \n",
    "        ELBO = ELBO_mask_SGA(i_low_type, i_hig_type, ELBO, variational_dictionary)\n",
    "        if (mean_S != 0) and (~np.all(np.isnan(ELBO))):\n",
    "            if learning_rate == \"RMSProp\":\n",
    "                rho, avg_new = RMSProp(avg_old = avg_old, gradient = ELBO, decay = decay, eta = eta , eps = eps)\n",
    "            elif learning_rate == \"AdaGrad\":\n",
    "                rho, diag_G_prop = AdaGrad(diag_G = diag_G, gradient = ELBO, eta = eta, eps = eps)\n",
    "            \n",
    "            if learning_rate == \"Adam\":\n",
    "                proposal, m_new, v_new = Adam(m_old = m_old, v_old = v_old, gradient = ELBO, step_counter = step_counter,\n",
    "                                              decay = decay, eta = eta , eps = eps)\n",
    "                var_lambda_prop = var_lambda + proposal\n",
    "            else:\n",
    "                var_lambda_prop = var_lambda + rho * ELBO\n",
    "            var_lambda = var_lambda_prop\n",
    "            \n",
    "            \n",
    "            time_counter = np.append(time_counter, time.monotonic())\n",
    "                \n",
    "            if learning_rate == \"RMSProp\":\n",
    "                    #RMSProp\n",
    "                avg_old = avg_new\n",
    "            elif learning_rate == \"AdaGrad\":\n",
    "                    # AdaGrad\n",
    "                diag_G = diag_G_prop\n",
    "            elif learning_rate == \"Basic\":\n",
    "                    # stepsize update\n",
    "                t = t + 1\n",
    "                rho = t ** (alpha)\n",
    "            elif learning_rate == \"Adam\":\n",
    "                m_old = m_new\n",
    "                v_old = v_new\n",
    "                step_counter = step_counter + 1\n",
    "        else:\n",
    "            print(\"here\")\n",
    "            time_counter = np.append(time_counter, time.monotonic())            \n",
    "        \n",
    "        var_lambda_array = var_lambda_array.append(pd.DataFrame(var_lambda.reshape(1, len(indexes)), columns=indexes))\n",
    "        # Change this modulo setting based how often you want the samples to be saved\n",
    "        if (steps % 10) == 9:\n",
    "            np.save(os.getcwd() + \"/\" + folder + \"/\" + \"param_nobs_\" + str(n_obs) + \"_S_\" + str(S) + \"_l_\" + str(l) +  \"_vine_\" + vine_type + \"_step_\" + str(n_steps), var_lambda_array)\n",
    "            np.save(os.getcwd() + \"/\" + folder + \"/\" + \"time_nobs_\" + str(n_obs) + \"_S_\" + str(S) + \"_l_\" + str(l) + \"_vine_\" + vine_type + \"_step_\" + str(n_steps), time_counter)\n",
    "    \n",
    "    return time_counter, var_lambda_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Variational Calibration - Rao-Blackwellization + Control Variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#### Interface for Variational Calibration with Rao-Blackwellization and CV\n",
    "# Make sure you run the chunk \"Variational Calibration - Rao-Blackwellization\" prior this because it contains\n",
    "# some utility functions needed here\n",
    "\n",
    "def VC_calibration_hyper_RB_CV(data_input, Y_index, Z_index, input_m, kernels,\n",
    "                         means, vine_type, priors_dictionary, variational_dictionary, x_dim, theta_dim, n_steps,\n",
    "                         S, S_CV, l, folder, learning_rate = \"RMSProp\", n_core = 1, eta = 0.01, decay = 1 / 2):\n",
    "    \"\"\"Scalable variational calibration of computer models according to Kejzlar and Maiti (2020).\n",
    "    \n",
    "    - Rao-Blackwellization + Control Variates\n",
    "    \n",
    "    Args:\n",
    "        data_input: A dictionary containing 3 items: Response, X, theta for each of data points\n",
    "        Y_index: Index of data points in range 1,..., len(Y)\n",
    "        Z_index: Index of data points in range len(Y) + 1, n\n",
    "        input_m: Ndarray with model inputs.\n",
    "        kernels: A dictionary containing 3 items: {\"f\": string describing the type of kernel,\n",
    "                 \"delta\": string describing the type of kernel, \"noise\": current noise value as float}\n",
    "        means: A dictionary containing 2 items: f, delta\n",
    "               {\"f\": string desciribing the type of mean, \"delta\": string describing the type of mean}\n",
    "               note: if there are no priors present for the mean parameters, the dictionary contains instances \n",
    "               of model_mean and delta_mean classes\n",
    "        vine_type: \"C\" or \"D\" depending on which wine copule we want to consider\n",
    "        theta_sample_dictionary: A sample from the variational family as a dictionary containing the keys\n",
    "                                {\"theta\", \"sigma\", \"kernel_f_l\", \"kernel_f_eta\", \"kernel_delta_l\", \"kernel_delta_eta\"}\n",
    "                                and \"mean_f\" or \"mean_delta\" if the mean hyperparameters are not fixed.\n",
    "        priors_dictionary: A dictionary of instances of gamma_mean_field_family and gaussian_mean_field_family\n",
    "                            in a dictionary with keys {\"theta\", \"sigma\", \"kernel_f_l\", \"kernel_f_eta\",\n",
    "                            \"kernel_delta_l\", \"kernel_delta_eta\"} and \"mean_f\" or \"mean_delta\" if the\n",
    "                            mean hyperparameters are not fixed.\n",
    "        variational_dictionary: A dictionary of instances of gamma_mean_field_family and gaussian_mean_field_family\n",
    "                            in a dictionary with keys {\"theta\", \"sigma\", \"kernel_f_l\", \"kernel_f_eta\",\n",
    "                            \"kernel_delta_l\", \"kernel_delta_eta\"} and \"mean_f\" or \"mean_delta\" if the\n",
    "                            mean hyperparameters are not fixed.\n",
    "        x_dim: The dimension of model inputs.\n",
    "        theta_dim: The dimension of calibraiton parameters.\n",
    "        n_steps: The number of evolutions of SGA to be performed.\n",
    "        S: Numer of samples from the variational family to be taken in the MCMC approximation of ELBO\n",
    "        S_cv: number of independet samples to be generated in order to compute control variates\n",
    "        l: Truncation level\n",
    "        folder: The string with path to the folder where the results would be stored\n",
    "                - stores array of variatioanal parameters and time stamps\n",
    "                - saves every 10 iterations - this can be modified at the end of the function def.\n",
    "        learning_rate: The learning reate algorithm to be used in SGA: \"Basic,\" \"AdaGrad,\" \"RMSProp,\" or \"Adam\"\n",
    "        n_core: The number of cores to be used for the parallel evaluation of MCMC integrals\n",
    "        eta: Innitial learning rate\n",
    "        decay: The value of forgetting factor to be used in either \"RMSProp\" or \"Adam\"\n",
    "    Returns:\n",
    "        time_counter: An array of system time of each evolution step of the SGA\n",
    "        var_lambda_array: A Pandas DataFrame of the evolution of variational parameters during the optimization.\n",
    "                          Each row correspond to a single evolution step\n",
    "    \"\"\"\n",
    "    \n",
    "    time_counter = np.array(time.monotonic())\n",
    "    \n",
    "    n_m = input_m.shape[0]\n",
    "    n_obs = data_input[\"Response\"].shape[0] - n_m\n",
    "    n = n_m + n_obs\n",
    "    bijection_domain = int(round(l * (2 * n - (l + 1)) / 2))\n",
    "    \n",
    "    ##### Innitial variational parameters\n",
    "    var_lambda, indexes = variational_doctioinary_to_lambda(variational_dictionary)\n",
    "    var_lambda_array = pd.DataFrame(var_lambda.reshape(1, len(indexes)), columns=indexes)\n",
    "    # Creating folder to save results\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    else:\n",
    "        shutil.rmtree(folder)           #removes all the subdirectories!\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    # Learning rate preset\n",
    "    if learning_rate == \"RMSProp\":\n",
    "        # RMSProp preset\n",
    "        eps = 1 / (10 ** 6)\n",
    "        avg_old = np.zeros(len(var_lambda))\n",
    "    elif learning_rate == \"AdaGrad\":   \n",
    "        # AdaGrad preset to default values\n",
    "        eps = 1 / (10 ** 6)\n",
    "        diag_G = np.zeros(len(var_lambda))\n",
    "    elif learning_rate == \"Basic\":\n",
    "        # Fixed step size - Do not recommend\n",
    "        eps = eta\n",
    "        alpha = - 0.75\n",
    "        t = np.round((eps / bijection_domain) ** (1 / alpha)) # step counter\n",
    "        rho = t ** (alpha)\n",
    "    elif learning_rate == \"Adam\":\n",
    "        eps = 1 / (10 ** 6)\n",
    "        m_old = np.zeros(len(var_lambda))\n",
    "        v_old = np.zeros(len(var_lambda))\n",
    "        step_counter = 1\n",
    "    \n",
    "    # the main VD algorithm\n",
    "    eta_original = eta\n",
    "    for steps in tqdm(range(n_steps), desc = \"SGA optimization\"):\n",
    "        if steps < 10:\n",
    "            eta = eta_original / 10\n",
    "        else:\n",
    "            eta = eta_original\n",
    "        \n",
    "        # Bijection\n",
    "        k = np.random.randint(1, bijection_domain + 1)\n",
    "        i, j = bijection_array(k, n, l)\n",
    "\n",
    "        #### Variational defintion family at the current value of the variational parameter\n",
    "        variational_dictionary = var_lambda_to_variational_dictionary(variational_dictionary, var_lambda, indexes)\n",
    "        \n",
    "        #### Sampling from the variational family for MCMC approximation \n",
    "        theta_sample_dictionary_MCMC = []\n",
    "        for s in range(S + S_CV):\n",
    "            theta_sample_dictionary = {}\n",
    "            for item in variational_dictionary:\n",
    "                theta_sample_dictionary[item] = variational_dictionary[item].sample(1).flatten()\n",
    "            theta_sample_dictionary_MCMC = theta_sample_dictionary_MCMC + [theta_sample_dictionary]\n",
    "        \n",
    "        ### Kernels and means\n",
    "        ELBO = 0\n",
    "        ELBO_add_array, h_array = zip(*Parallel(n_jobs = n_core)(delayed(delta_ELBO_hyper_RB)(data_input, Y_index,\n",
    "                                                                                           Z_index, input_m, means,\n",
    "                                                                                           kernels, vine_type,\n",
    "                                                                                           theta_sample_dictionary,\n",
    "                                                                                           priors_dictionary,\n",
    "                                                                                           variational_dictionary,\n",
    "                                                                                           n_obs, x_dim, theta_dim, i,\n",
    "                                                                                           j, l, n) for theta_sample_dictionary in theta_sample_dictionary_MCMC))\n",
    "        \n",
    "        # Control Variates handling\n",
    "        ELBO_add_array = np.asarray(ELBO_add_array).T\n",
    "        h_array = np.asarray(h_array).T\n",
    "                \n",
    "        nan_mask = ~np.all(np.isnan(h_array), axis = 0)\n",
    "        h_array = h_array[:, nan_mask]\n",
    "        ELBO_add_array = ELBO_add_array[:, nan_mask]\n",
    "        \n",
    "        # Separation of data into CV estimates part and ELBO estimates part\n",
    "        h_array_CV = h_array[:,(-S_CV):]\n",
    "        ELBO_add_array_CV = ELBO_add_array[:,(-S_CV):]\n",
    "        h_array = h_array[:, :(-S_CV)]\n",
    "        ELBO_add_array = ELBO_add_array[:, :(-S_CV)]\n",
    "        \n",
    "        mean_S = h_array.shape[1]\n",
    "\n",
    "        # Estimate of control variete\n",
    "        num = np.sum((h_array_CV - np.mean(h_array_CV, axis= 1)[:,None]) * (ELBO_add_array_CV - np.mean(ELBO_add_array_CV, axis= 1)[:,None]), axis = 1) / (ELBO_add_array_CV.shape[1] - 1)\n",
    "        denom = np.var(h_array_CV, axis= 1)\n",
    "        a = num / denom\n",
    "        \n",
    "        ELBO = (np.sum(ELBO_add_array, axis = 1) - a * np.sum(h_array, axis = 1)) / mean_S\n",
    "        \n",
    "        ############################################\n",
    "        if vine_type == \"D\":\n",
    "            i_low = i\n",
    "            i_hig = i + j\n",
    "        elif vine_type == \"C\":\n",
    "            i_low = j\n",
    "            i_hig = i + j\n",
    "            \n",
    "        i_low_type = data_input[\"Response\"][i_low - 1,:][1]\n",
    "        i_hig_type = data_input[\"Response\"][i_hig - 1,:][1]\n",
    "        \n",
    "        \n",
    "        ELBO = ELBO_mask_SGA(i_low_type, i_hig_type, ELBO, variational_dictionary)\n",
    "\n",
    "        if (mean_S != 0) and (~np.all(np.isnan(ELBO))):\n",
    "            if learning_rate == \"RMSProp\":\n",
    "                rho, avg_new = RMSProp(avg_old = avg_old, gradient = ELBO, decay = decay, eta = eta , eps = eps)\n",
    "            elif learning_rate == \"AdaGrad\":\n",
    "                rho, diag_G_prop = AdaGrad(diag_G = diag_G, gradient = ELBO, eta = eta, eps = eps)\n",
    "            \n",
    "        # Validity check for proposal\n",
    "            if learning_rate == \"Adam\":\n",
    "                proposal, m_new, v_new = Adam(m_old = m_old, v_old = v_old, gradient = ELBO, step_counter = step_counter,\n",
    "                                              decay = decay, eta = eta , eps = eps)\n",
    "                var_lambda_prop = var_lambda + proposal\n",
    "            else:\n",
    "                var_lambda_prop = var_lambda + rho * ELBO\n",
    "            var_lambda = var_lambda_prop\n",
    "            \n",
    "            \n",
    "            time_counter = np.append(time_counter, time.monotonic())\n",
    "                \n",
    "            if learning_rate == \"RMSProp\":\n",
    "                    #RMSProp\n",
    "                avg_old = avg_new\n",
    "            elif learning_rate == \"AdaGrad\":\n",
    "                    # AdaGrad\n",
    "                diag_G = diag_G_prop\n",
    "            elif learning_rate == \"Basic\":\n",
    "                    # stepsize update\n",
    "                t = t + 1\n",
    "                rho = t ** (alpha)\n",
    "            elif learning_rate == \"Adam\":\n",
    "                m_old = m_new\n",
    "                v_old = v_new\n",
    "                step_counter = step_counter + 1\n",
    "        else:\n",
    "            print(\"here\")\n",
    "            time_counter = np.append(time_counter, time.monotonic())\n",
    "                \n",
    "        # Change this modulo setting based how often you want the samples to be saved\n",
    "        var_lambda_array = var_lambda_array.append(pd.DataFrame(var_lambda.reshape(1, len(indexes)), columns=indexes))\n",
    "        if (steps % 10) == 9:\n",
    "            np.save(os.getcwd() + \"/\" + folder + \"/\" + \"param_nobs_\" + str(n_obs) + \"_S_\" + str(S) + \"_l_\" + str(l) +  \"_vine_\" + vine_type + \"_step_\" + str(n_steps), var_lambda_array)\n",
    "            np.save(os.getcwd() + \"/\" + folder + \"/\" + \"time_nobs_\" + str(n_obs) + \"_S_\" + str(S) + \"_l_\" + str(l) + \"_vine_\" + vine_type + \"_step_\" + str(n_steps), time_counter)\n",
    "    \n",
    "    return time_counter, var_lambda_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Calibration - Rao-Blackwellization + Control Variates + Imoprtance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
